{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "388fb329",
   "metadata": {
    "id": "388fb329"
   },
   "source": [
    "# Title: WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off\n",
    "\n",
    "#### Members' Names : Burhan Maseel & Lakshita Mahajan\n",
    "\n",
    "#### Emails: burhan.maseel@torontomu.ca & lakshita.mahajan@torontomu.ca\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "#### Problem Description:\n",
    "Watermarking has become a very critical tool in nullyfying the misuse of Large Language Models (LLMs), as these models can generate text that mimics human writing. There are a huge amount of risks associated, including pretending to be someone else and the generation of fake news. To combat these threats, the ability to trace the origin of generated text is crucial. As LLMs are increasingly used in academic and professional settings, this raises concerns about cheating, plagiarism, and authenticity. For instance, college students might use LLMs to generate essays, reports, or assignments without proper citation, undermining academic integrity. Similarly, in interviews, candidates may use LLMs to generate responses that appear original, leading to unfair advantages and misrepresentations of their actual abilities.\n",
    "\n",
    "#### Context of the Problem:\n",
    "Existing watermarking techniques aim to embed imperceptible signals within the text generated by an LLM, allowing the text's origin to be identified later through a secret key. However, many of these methods face a significant trade-off between detectability and text quality. The current literature reveals that while passive methods for text forensics are versatile, they offer low performance with error rates that rarely dip below 10^-3. Active methods like watermarking, though more effective, typically introduce distortions in the text to achieve higher detectability, which compromises the quality of the generated content.\n",
    "\n",
    "The rise of LLMs has further exacerbated the issue of plagiarism and cheating in education. As these models can generate human-like text on demand, students might exploit them to bypass the effort required for assignments, ultimately degrading the learning process and academic standards. In interviews, candidates might use LLMs to craft responses that appear intelligent and insightful but lack personal experience or understanding. This creates an unfair scenario where the candidate’s true capabilities are hidden, leading to unjust outcomes in hiring decisions.\n",
    "\n",
    "The ability to detect such misuse through watermarking is essential, but it must be done without compromising the quality of the generated content. As a result, a reliable and robust watermarking technique is needed to address the growing concerns around cheating, plagiarism, and the overall integrity of systems that rely on generated content.\n",
    "\n",
    "\n",
    "# 2. Methodology\n",
    "\n",
    "WaterMax introduces a black-box watermarking strategy that ensures robustness and fluency without accessing the internal workings of the language model.\n",
    "\n",
    "## Watermark Score Function\n",
    "\n",
    "WaterMax defines a **scoring function** that evaluates how well a generated chunk aligns with a seeded statistical signature. This function enables the system to rank candidate chunks based on how detectable and consistent their watermark signals are.\n",
    "\n",
    "The watermark score \\( S \\) for a chunk is calculated as:\n",
    "\n",
    "\\[\n",
    "S = \\sum_{i=1}^{n} \\delta\\left( \\text{chunk}_i \\oplus \\text{key}_i \\right)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\delta \\) returns 1 if the tokens match and 0 otherwise.\n",
    "- \\( n \\) is the number of tokens in the chunk.\n",
    "- \\( \\text{chunk}_i \\) and \\( \\text{key}_i \\) represent the i-th token in the chunk and the watermark key.\n",
    "\n",
    "## Exploring the Text Space\n",
    "\n",
    "Instead of modifying token distributions, WaterMax performs **draft-and-select**:\n",
    "\n",
    "- Multiple candidate chunks are sampled using the base LLM.\n",
    "- The scoring function evaluates each candidate chunk.\n",
    "- The chunk with the best score is selected.\n",
    "- This process repeats chunk by chunk until the full text is generated.\n",
    "\n",
    "This allows WaterMax to embed watermarks in a **black-box setting** without sacrificing text quality.\n",
    "\n",
    "## Detection Strategy\n",
    "\n",
    "Watermark detection is performed using a **likelihood ratio test (LRT)**:\n",
    "\n",
    "\\[\n",
    "P_{\\text{watermark}} = \\frac{\\text{score of watermark chunk}}{\\text{score of random chunk}}\n",
    "\\]\n",
    "\n",
    "- This test compares the probability of the text being generated with vs. without the watermark under the known seed.\n",
    "- If the ratio exceeds a threshold, the text is classified as watermarked.\n",
    "\n",
    "## Robustness to Adversarial Attacks\n",
    "\n",
    "WaterMax enhances robustness through **chunk-based watermarking**:\n",
    "\n",
    "- Watermarks are independently embedded in each chunk.\n",
    "- Local edits (e.g., paraphrasing) only affect some chunks, leaving others intact.\n",
    "- This design improves resistance against common attacks compared to continuous watermarking.\n",
    "\n",
    "## Tuning the Trade-off between Fidelity and Detectability\n",
    "\n",
    "WaterMax introduces a **tuning knob** that balances watermark strength and fluency:\n",
    "\n",
    "- More drafts = stronger watermark signal, slightly lower fluency.\n",
    "- Fewer drafts = higher fluency, slightly weaker watermark.\n",
    "\n",
    "This trade-off allows adapting the watermarking strategy based on application needs.\n",
    "\n",
    "\n",
    "\n",
    "# 3. Solution\n",
    "\n",
    "In this section, we outline the solution provided by **WaterMax** for embedding a watermark into text generated by Large Language Models (LLMs). The approach leverages a detector-first design that maximizes detection power while ensuring high-quality, fluent text generation. Below, we explain the key components of the solution.\n",
    "\n",
    "## 3.1. Watermark Embedding Strategy\n",
    "\n",
    "**WaterMax** employs a robust watermark embedding strategy that is based on a **chunk-level selection process**. This process enables embedding watermarks without modifying the core architecture or components of the LLM, ensuring the model's functionality remains intact.\n",
    "\n",
    "### Key Steps in Watermark Embedding:\n",
    "\n",
    "- **Chunk Generation**: For each segment of the text, multiple candidate chunks are generated using the LLM.\n",
    "- **Watermark Score Calculation**: Each candidate chunk is evaluated using the **watermark score function**, which calculates the similarity between the chunk and a pre-defined watermark key.\n",
    "- **Best Chunk Selection**: The chunk with the highest watermark score is selected, ensuring that the watermark is embedded without sacrificing fluency.\n",
    "- **Iterative Process**: This chunk-based embedding process is repeated for each text segment, ensuring that the watermark is efficiently embedded in the entire text.\n",
    "\n",
    "This process guarantees that the watermark is seamlessly embedded without requiring internal model modifications, preserving the LLM's original performance and output quality.\n",
    "\n",
    "## 3.2. Detection Mechanism\n",
    "\n",
    "Once the watermark is embedded, it can be detected using a **likelihood ratio test (LRT)**. This test allows for the identification of the watermark in the text without requiring access to the internal parameters of the LLM.\n",
    "\n",
    "### Likelihood Ratio Test for Watermark Detection:\n",
    "\n",
    "- **Test Functionality**: The LRT compares the likelihood of the text being generated with or without the watermark under the known seeded key.\n",
    "- **No Need for Model Access**: Unlike traditional methods, **WaterMax** does not require access to the LLM's internals (such as logits or weights).\n",
    "- **Detection Formula**: The detection is based on the following formula:\n",
    "  \n",
    "  \\[\n",
    "  P_{\\text{watermark}} = \\frac{\\text{score of watermark chunk}}{\\text{score of random chunk}}\n",
    "  \\]\n",
    "\n",
    "  If the ratio exceeds a defined threshold, the text is flagged as watermarked.\n",
    "\n",
    "## 3.3. Robustness and Scalability\n",
    "\n",
    "**WaterMax** is designed to be robust against various adversarial attacks such as paraphrasing or text manipulation. The use of independent chunks ensures that partial changes in the text do not affect the entire watermark, making it resilient to local edits.\n",
    "\n",
    "### Advantages of WaterMax's Robustness:\n",
    "\n",
    "- **Chunk-Based Design**: The watermark is embedded across multiple independent chunks of text, increasing robustness.\n",
    "- **Local Impact of Edits**: Any changes or edits to individual chunks affect only those specific chunks, preserving the watermark in the rest of the text.\n",
    "- **Resiliency to Paraphrasing**: The chunk-based design makes it harder for attackers to remove or alter the watermark without significantly degrading the overall text quality.\n",
    "\n",
    "In addition, **WaterMax** is highly scalable, able to handle large volumes of text generation without significant loss in performance or detection accuracy. The computational complexity remains manageable thanks to modern GPU parallelization techniques.\n",
    "\n",
    "## 3.4. Key Benefits of the WaterMax Solution\n",
    "\n",
    "The **WaterMax** watermarking scheme offers several notable benefits that differentiate it from previous approaches:\n",
    "\n",
    "- **High Detection Accuracy**: Even on short texts, **WaterMax** achieves near-perfect detection accuracy.\n",
    "- **Minimal Impact on Text Quality**: Unlike previous methods, **WaterMax** ensures that the watermark embedding process does not degrade the fluency or readability of the text.\n",
    "- **No Model Modification**: **WaterMax** operates as a black-box approach, ensuring that the LLM’s core functionality remains unchanged, allowing for easy integration with pre-existing models.\n",
    "- **Scalability**: The solution is highly scalable and can be applied to large datasets or real-time text generation scenarios.\n",
    "\n",
    "**WaterMax** stands out as a robust, efficient, and scalable solution for watermarking text generated by LLMs, offering high detection rates while maintaining text quality.\n",
    "\n",
    "# 4. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee863785",
   "metadata": {},
   "source": [
    "Setting up Colab Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85114758",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20868,
     "status": "ok",
     "timestamp": 1744563910296,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "85114758",
    "outputId": "fe02ffb4-1429-45cb-8735-b32ad65f49e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import sys\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "py_file_location = \"/content/drive/MyDrive/NLP Projects/WaterMax\"\n",
    "sys.path.append(os.path.abspath(py_file_location))\n",
    "\n",
    "os.chdir(py_file_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8153ddeb",
   "metadata": {},
   "source": [
    "Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "otgHas2ptIQj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 90195,
     "status": "ok",
     "timestamp": 1744567026924,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "otgHas2ptIQj",
    "outputId": "d7f8e01d-52c3-41eb-9022-653e5e8d4caa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dacite==1.8.1 (from -r ./src/requirements.txt (line 1))\n",
      "  Downloading dacite-1.8.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting evaluate>=0.4.1 (from -r ./src/requirements.txt (line 2))\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: numpy>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from -r ./src/requirements.txt (line 3)) (2.0.2)\n",
      "Collecting pandas==1.5.3 (from -r ./src/requirements.txt (line 4))\n",
      "  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy==1.11.1 (from -r ./src/requirements.txt (line 5))\n",
      "  Downloading scipy-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from -r ./src/requirements.txt (line 6)) (2.6.0+cu124)\n",
      "Collecting tqdm==4.66.3 (from -r ./src/requirements.txt (line 7))\n",
      "  Downloading tqdm-4.66.3-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: transformers>=4.31 in /usr/local/lib/python3.11/dist-packages (from -r ./src/requirements.txt (line 8)) (4.50.3)\n",
      "Collecting mauve-text (from -r ./src/requirements.txt (line 9))\n",
      "  Downloading mauve_text-0.4.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rouge-score (from -r ./src/requirements.txt (line 10))\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from -r ./src/requirements.txt (line 11)) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3->-r ./src/requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3->-r ./src/requirements.txt (line 4)) (2025.2)\n",
      "Collecting numpy>=1.24.3 (from -r ./src/requirements.txt (line 3))\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets>=2.0.0 (from evaluate>=0.4.1->-r ./src/requirements.txt (line 2))\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting dill (from evaluate>=0.4.1->-r ./src/requirements.txt (line 2))\n",
      "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate>=0.4.1->-r ./src/requirements.txt (line 2)) (2.32.3)\n",
      "Collecting xxhash (from evaluate>=0.4.1->-r ./src/requirements.txt (line 2))\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from evaluate>=0.4.1->-r ./src/requirements.txt (line 2))\n",
      "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate>=0.4.1->-r ./src/requirements.txt (line 2)) (2025.3.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate>=0.4.1->-r ./src/requirements.txt (line 2)) (0.30.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate>=0.4.1->-r ./src/requirements.txt (line 2)) (24.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->-r ./src/requirements.txt (line 6)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->-r ./src/requirements.txt (line 6)) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->-r ./src/requirements.txt (line 6)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->-r ./src/requirements.txt (line 6)) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.1->-r ./src/requirements.txt (line 6))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.1->-r ./src/requirements.txt (line 6))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.1->-r ./src/requirements.txt (line 6))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.1->-r ./src/requirements.txt (line 6))\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.1->-r ./src/requirements.txt (line 6))\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.1->-r ./src/requirements.txt (line 6))\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.1->-r ./src/requirements.txt (line 6))\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.1->-r ./src/requirements.txt (line 6))\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.1->-r ./src/requirements.txt (line 6))\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->-r ./src/requirements.txt (line 6)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->-r ./src/requirements.txt (line 6)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->-r ./src/requirements.txt (line 6)) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.1->-r ./src/requirements.txt (line 6))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->-r ./src/requirements.txt (line 6)) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->-r ./src/requirements.txt (line 6)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.1->-r ./src/requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31->-r ./src/requirements.txt (line 8)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31->-r ./src/requirements.txt (line 8)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31->-r ./src/requirements.txt (line 8)) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31->-r ./src/requirements.txt (line 8)) (0.5.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.11/dist-packages (from mauve-text->-r ./src/requirements.txt (line 9)) (1.6.1)\n",
      "Collecting faiss-cpu>=1.7.0 (from mauve-text->-r ./src/requirements.txt (line 9))\n",
      "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score->-r ./src/requirements.txt (line 10)) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score->-r ./src/requirements.txt (line 10)) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score->-r ./src/requirements.txt (line 10)) (1.17.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->-r ./src/requirements.txt (line 11)) (5.9.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate>=0.4.1->-r ./src/requirements.txt (line 2)) (18.1.0)\n",
      "Collecting dill (from evaluate>=0.4.1->-r ./src/requirements.txt (line 2))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting multiprocess (from evaluate>=0.4.1->-r ./src/requirements.txt (line 2))\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate>=0.4.1->-r ./src/requirements.txt (line 2))\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate>=0.4.1->-r ./src/requirements.txt (line 2)) (3.11.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate>=0.4.1->-r ./src/requirements.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate>=0.4.1->-r ./src/requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate>=0.4.1->-r ./src/requirements.txt (line 2)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate>=0.4.1->-r ./src/requirements.txt (line 2)) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.1->mauve-text->-r ./src/requirements.txt (line 9)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.1->mauve-text->-r ./src/requirements.txt (line 9)) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.1->-r ./src/requirements.txt (line 6)) (3.0.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score->-r ./src/requirements.txt (line 10)) (8.1.8)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate>=0.4.1->-r ./src/requirements.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate>=0.4.1->-r ./src/requirements.txt (line 2)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate>=0.4.1->-r ./src/requirements.txt (line 2)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate>=0.4.1->-r ./src/requirements.txt (line 2)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate>=0.4.1->-r ./src/requirements.txt (line 2)) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate>=0.4.1->-r ./src/requirements.txt (line 2)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate>=0.4.1->-r ./src/requirements.txt (line 2)) (1.18.3)\n",
      "Downloading dacite-1.8.1-py3-none-any.whl (14 kB)\n",
      "Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m122.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.3-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mauve_text-0.4.0-py3-none-any.whl (21 kB)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=314d6de37c27779f2c6ccd16abe266ace79f2b31f8256d8a61bc2f85687d4ea2\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: xxhash, tqdm, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, fsspec, dill, dacite, scipy, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, faiss-cpu, rouge-score, nvidia-cusolver-cu12, mauve-text, datasets, evaluate\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.2\n",
      "    Uninstalling pandas-2.2.2:\n",
      "      Successfully uninstalled pandas-2.2.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
      "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
      "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
      "xarray 2025.1.2 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
      "mizani 0.13.2 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.11.1 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
      "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
      "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dacite-1.8.1 datasets-3.5.0 dill-0.3.8 evaluate-0.4.3 faiss-cpu-1.10.0 fsspec-2024.12.0 mauve-text-0.4.0 multiprocess-0.70.16 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pandas-1.5.3 rouge-score-0.1.2 scipy-1.11.1 tqdm-4.66.3 xxhash-3.5.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "e111f8a2332b4887897fb723acd0d5d2",
       "pip_warning": {
        "packages": [
         "tqdm"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -r ./src/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "TYgX7AMUtOq4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9740,
     "status": "ok",
     "timestamp": 1744564067103,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "TYgX7AMUtOq4",
    "outputId": "e41ad867-4c2c-4c3a-88d9-c30af266609d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook already exists: pre-push\n",
      "\n",
      "\t#!/bin/sh\n",
      "\tcommand -v git-lfs >/dev/null 2>&1 || { printf >&2 \"\\n%s\\n\\n\" \"This repository is configured for Git LFS but 'git-lfs' was not found on your path. If you no longer wish to use Git LFS, remove this hook by deleting the 'pre-push' file in the hooks directory (set by 'core.hookspath'; usually '.git/hooks').\"; exit 2; }\n",
      "\tgit lfs pre-push \"$@\"\n",
      "\n",
      "To resolve this, either:\n",
      "  1: run `git lfs update --manual` for instructions on how to merge hooks.\n",
      "  2: run `git lfs update --force` to overwrite your hook.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vKJquwc_clQ4",
   "metadata": {
    "id": "vKJquwc_clQ4"
   },
   "source": [
    "Make sure to configure your SSH key with hugging face before running the clone model code cell. To test whether your ssh key is configured properly run the ssh command. If ssh is configured properly you'll get the message \"Hi [username], welcome to Hugging Face.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zWaOJ6HmtWFH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7540,
     "status": "ok",
     "timestamp": 1744565999896,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "zWaOJ6HmtWFH",
    "outputId": "c4289a2b-7835-43f5-9ce9-471752eac211"
   },
   "outputs": [],
   "source": [
    "!ssh-keygen -t rsa -b 4096 -C \"burhan.maseel@torontomu.ca\"\n",
    "!cat /root/.ssh/id_rsa.pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "GA0wNNOEttVa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1015,
     "status": "ok",
     "timestamp": 1744566472355,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "GA0wNNOEttVa",
    "outputId": "e687698e-4631-4a95-c79e-448714782049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# github.com:22 SSH-2.0-85aa1254\n"
     ]
    }
   ],
   "source": [
    "!ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "qKgqUvoid2QT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2515,
     "status": "ok",
     "timestamp": 1744566501704,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "qKgqUvoid2QT",
    "outputId": "bc5a71db-5c77-4a0a-e42f-211d064541ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi anonymous, welcome to Hugging Face.\n"
     ]
    }
   ],
   "source": [
    "!ssh -T git@hf.co"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488a2e28",
   "metadata": {},
   "source": [
    "Once the access to Llama-2 model is approved we have to clone the model locally to generate the watermarked text using the Llama-2-7b-chat-hf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1874873f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 121,
     "status": "ok",
     "timestamp": 1744566504273,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "1874873f",
    "outputId": "7d54a5b0-4b48-4da9-a074-230aa8ff9d52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'src/Llama-2-7b-chat-hf' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone git@hf.co:meta-llama/Llama-2-7b-chat-hf src/Llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104973a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "c002ea2a995d4cb1947bb0f61aeaed37",
      "18831298d59847d8b1a03c1ab2bec5b9",
      "27daae7f37cd451fa3e33f85fa5cddbe",
      "35020a4b4de444ddae141a591f36bc77",
      "9d3b7be89ccc43a1b22de097f06e115e",
      "228176a48ae442b5aabae1fda43be108",
      "add3e73ced284765b8fcc150c09d9378",
      "66bbe7f680634504846a9d98cc4896ab",
      "d600e11cd88a4e45913c0b6abaab2809",
      "2b19a50a5cd24383a832857e46ba008d",
      "d158fd8f46114f5c932a6e3bba4b9767",
      "a258523e6acf4ff99b5756ff9b5821a7",
      "548d20591c7243de9dd628fa98417391",
      "bec6b5f945a2469e9153ac89739496d3",
      "3b30b6d4c6814886989d23a9489ac141",
      "e44dc3b705a0442ea818b9415cd7398d",
      "558fbf374a344e0fb9415ab717933477",
      "802cc0afab1b4b44a4dea599ad1d08d3",
      "6212676f7041412e8d170dd8658f6908",
      "65c8b966a4b04b8a953aaea84f5e61bf"
     ]
    },
    "executionInfo": {
     "elapsed": 519,
     "status": "ok",
     "timestamp": 1744566509052,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "104973a3",
    "outputId": "46a20660-2e73-4ea1-e17c-eafe0420b138"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff07ae2",
   "metadata": {},
   "source": [
    "Run the following command to generate the text using the test prompts from a text file from data/test_prompts.txt. Our generated text is stored in results files and the score and p values are stored in the scores files in interactive_prompting directory under the relevant model directory used to generate the text. Here in the followed cells we are printing just the top 3 results from the files but the complete results and scores can be accessed from the results directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N1RJrMqTw0dt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 86183,
     "status": "ok",
     "timestamp": 1744514626816,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "N1RJrMqTw0dt",
    "outputId": "14dcc692-499d-4713-e229-1ce064fab3e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-13 03:22:38.589575: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-13 03:22:39.189714: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744514559.421380    5347 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744514559.481704    5347 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-13 03:22:39.999236: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "tokenizer_config.json: 100% 1.62k/1.62k [00:00<00:00, 10.8MB/s]\n",
      "tokenizer.model: 100% 500k/500k [00:00<00:00, 63.0MB/s]\n",
      "tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 42.3MB/s]\n",
      "special_tokens_map.json: 100% 414/414 [00:00<00:00, 2.86MB/s]\n",
      "config.json: 100% 614/614 [00:00<00:00, 3.60MB/s]\n",
      "model.safetensors.index.json: 100% 26.8k/26.8k [00:00<00:00, 93.5MB/s]\n",
      "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-00002.safetensors:   0% 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0% 41.9M/9.98G [00:00<00:28, 351MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1% 83.9M/9.98G [00:00<00:29, 339MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1% 126M/9.98G [00:00<00:28, 351MB/s] \u001b[A\n",
      "model-00001-of-00002.safetensors:   2% 168M/9.98G [00:00<00:27, 354MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2% 210M/9.98G [00:00<00:28, 337MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3% 262M/9.98G [00:00<00:26, 372MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3% 304M/9.98G [00:00<00:25, 377MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3% 346M/9.98G [00:00<00:26, 363MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4% 388M/9.98G [00:01<00:34, 280MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4% 419M/9.98G [00:01<00:36, 260MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5% 461M/9.98G [00:01<00:33, 282MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5% 493M/9.98G [00:01<00:32, 289MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5% 535M/9.98G [00:01<00:29, 318MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6% 587M/9.98G [00:01<00:27, 344MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6% 629M/9.98G [00:01<00:27, 339MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7% 682M/9.98G [00:02<00:27, 342MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7% 724M/9.98G [00:02<00:27, 341MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8% 765M/9.98G [00:02<00:26, 344MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8% 807M/9.98G [00:02<00:26, 345MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9% 849M/9.98G [00:02<00:28, 320MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9% 891M/9.98G [00:02<00:26, 340MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9% 933M/9.98G [00:02<00:27, 330MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10% 975M/9.98G [00:02<00:26, 341MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10% 1.03G/9.98G [00:03<00:23, 377MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11% 1.07G/9.98G [00:03<00:24, 367MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11% 1.11G/9.98G [00:03<00:24, 359MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12% 1.16G/9.98G [00:03<00:22, 384MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12% 1.22G/9.98G [00:03<00:21, 407MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13% 1.27G/9.98G [00:03<00:21, 413MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13% 1.32G/9.98G [00:03<00:20, 424MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14% 1.37G/9.98G [00:03<00:20, 427MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14% 1.43G/9.98G [00:04<00:19, 430MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15% 1.48G/9.98G [00:04<00:19, 431MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15% 1.53G/9.98G [00:04<00:19, 431MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16% 1.58G/9.98G [00:04<00:19, 435MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16% 1.64G/9.98G [00:04<00:19, 435MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17% 1.69G/9.98G [00:04<00:20, 406MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17% 1.73G/9.98G [00:04<00:20, 406MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18% 1.77G/9.98G [00:04<00:20, 396MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18% 1.81G/9.98G [00:04<00:20, 399MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19% 1.86G/9.98G [00:05<00:20, 396MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19% 1.90G/9.98G [00:05<00:20, 389MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20% 1.95G/9.98G [00:05<00:21, 372MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20% 1.99G/9.98G [00:05<00:24, 329MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20% 2.03G/9.98G [00:05<00:22, 347MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21% 2.09G/9.98G [00:05<00:21, 373MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21% 2.14G/9.98G [00:05<00:19, 398MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22% 2.19G/9.98G [00:05<00:18, 415MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22% 2.24G/9.98G [00:06<00:19, 399MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23% 2.30G/9.98G [00:06<00:18, 414MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23% 2.34G/9.98G [00:06<00:18, 403MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24% 2.38G/9.98G [00:06<00:21, 360MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24% 2.43G/9.98G [00:06<00:19, 380MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25% 2.49G/9.98G [00:06<00:18, 397MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25% 2.54G/9.98G [00:06<00:18, 410MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26% 2.59G/9.98G [00:06<00:17, 419MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26% 2.64G/9.98G [00:07<00:17, 428MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27% 2.69G/9.98G [00:07<00:17, 427MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28% 2.75G/9.98G [00:07<00:17, 420MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28% 2.80G/9.98G [00:07<00:17, 403MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29% 2.85G/9.98G [00:07<00:17, 406MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29% 2.89G/9.98G [00:07<00:17, 408MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29% 2.94G/9.98G [00:07<00:17, 397MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30% 2.98G/9.98G [00:07<00:18, 376MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30% 3.02G/9.98G [00:08<00:20, 338MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31% 3.06G/9.98G [00:08<00:20, 331MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31% 3.10G/9.98G [00:08<00:22, 304MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31% 3.14G/9.98G [00:08<00:23, 288MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32% 3.17G/9.98G [00:08<00:27, 248MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32% 3.21G/9.98G [00:08<00:24, 278MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33% 3.25G/9.98G [00:08<00:23, 291MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33% 3.29G/9.98G [00:09<00:21, 309MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33% 3.33G/9.98G [00:09<00:20, 328MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34% 3.38G/9.98G [00:09<00:19, 345MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34% 3.42G/9.98G [00:09<00:22, 293MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35% 3.45G/9.98G [00:09<00:23, 277MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35% 3.48G/9.98G [00:09<00:23, 280MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35% 3.53G/9.98G [00:09<00:21, 305MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36% 3.57G/9.98G [00:09<00:22, 286MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36% 3.60G/9.98G [00:10<00:23, 277MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36% 3.63G/9.98G [00:10<00:24, 256MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37% 3.66G/9.98G [00:10<00:24, 257MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37% 3.69G/9.98G [00:10<00:24, 261MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37% 3.72G/9.98G [00:10<00:24, 252MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38% 3.76G/9.98G [00:10<00:22, 271MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38% 3.80G/9.98G [00:10<00:23, 264MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38% 3.84G/9.98G [00:11<00:21, 288MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39% 3.88G/9.98G [00:11<00:20, 303MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39% 3.92G/9.98G [00:11<00:19, 307MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40% 3.95G/9.98G [00:11<00:26, 229MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40% 3.98G/9.98G [00:11<00:27, 215MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40% 4.02G/9.98G [00:11<00:25, 233MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41% 4.06G/9.98G [00:11<00:22, 265MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41% 4.10G/9.98G [00:12<00:20, 293MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42% 4.14G/9.98G [00:12<00:18, 316MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42% 4.18G/9.98G [00:12<00:17, 340MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42% 4.24G/9.98G [00:12<00:15, 369MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43% 4.28G/9.98G [00:12<00:15, 367MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43% 4.32G/9.98G [00:12<00:15, 376MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44% 4.36G/9.98G [00:12<00:15, 355MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44% 4.40G/9.98G [00:12<00:15, 366MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45% 4.45G/9.98G [00:12<00:16, 327MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45% 4.49G/9.98G [00:13<00:24, 222MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45% 4.53G/9.98G [00:13<00:21, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46% 4.58G/9.98G [00:13<00:17, 301MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46% 4.63G/9.98G [00:13<00:15, 345MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47% 4.68G/9.98G [00:13<00:15, 345MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47% 4.72G/9.98G [00:13<00:14, 356MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48% 4.76G/9.98G [00:13<00:14, 367MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48% 4.81G/9.98G [00:14<00:13, 388MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49% 4.85G/9.98G [00:14<00:13, 393MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49% 4.91G/9.98G [00:14<00:12, 405MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50% 4.96G/9.98G [00:14<00:11, 430MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50% 5.01G/9.98G [00:14<00:11, 418MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51% 5.06G/9.98G [00:14<00:11, 414MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51% 5.12G/9.98G [00:14<00:11, 426MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52% 5.17G/9.98G [00:14<00:11, 430MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52% 5.22G/9.98G [00:15<00:11, 404MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53% 5.26G/9.98G [00:15<00:13, 344MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53% 5.31G/9.98G [00:15<00:13, 336MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54% 5.35G/9.98G [00:15<00:14, 316MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54% 5.39G/9.98G [00:15<00:13, 329MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55% 5.44G/9.98G [00:15<00:12, 370MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55% 5.48G/9.98G [00:15<00:11, 376MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55% 5.54G/9.98G [00:15<00:11, 394MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56% 5.58G/9.98G [00:16<00:11, 383MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56% 5.63G/9.98G [00:16<00:10, 403MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57% 5.67G/9.98G [00:16<00:11, 389MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57% 5.73G/9.98G [00:16<00:10, 414MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58% 5.77G/9.98G [00:16<00:10, 388MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58% 5.81G/9.98G [00:16<00:10, 382MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59% 5.86G/9.98G [00:16<00:10, 403MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59% 5.90G/9.98G [00:16<00:10, 383MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60% 5.95G/9.98G [00:17<00:10, 375MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60% 5.99G/9.98G [00:17<00:10, 370MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60% 6.03G/9.98G [00:17<00:11, 354MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61% 6.07G/9.98G [00:17<00:11, 334MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61% 6.11G/9.98G [00:17<00:11, 329MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62% 6.16G/9.98G [00:17<00:11, 336MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62% 6.21G/9.98G [00:17<00:10, 359MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63% 6.25G/9.98G [00:17<00:10, 348MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63% 6.29G/9.98G [00:18<00:12, 286MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63% 6.32G/9.98G [00:18<00:14, 253MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64% 6.36G/9.98G [00:18<00:13, 274MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64% 6.41G/9.98G [00:18<00:11, 302MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65% 6.45G/9.98G [00:18<00:10, 329MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65% 6.49G/9.98G [00:18<00:09, 351MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65% 6.53G/9.98G [00:18<00:09, 348MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66% 6.59G/9.98G [00:18<00:08, 385MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67% 6.64G/9.98G [00:19<00:08, 373MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67% 6.68G/9.98G [00:19<00:08, 374MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67% 6.72G/9.98G [00:19<00:09, 330MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68% 6.76G/9.98G [00:19<00:09, 340MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68% 6.81G/9.98G [00:19<00:09, 349MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69% 6.85G/9.98G [00:19<00:08, 365MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69% 6.89G/9.98G [00:19<00:08, 379MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69% 6.93G/9.98G [00:19<00:08, 377MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70% 6.97G/9.98G [00:20<00:07, 384MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70% 7.03G/9.98G [00:20<00:07, 396MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71% 7.07G/9.98G [00:20<00:07, 388MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71% 7.11G/9.98G [00:20<00:07, 396MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72% 7.16G/9.98G [00:20<00:06, 408MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72% 7.20G/9.98G [00:20<00:07, 393MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73% 7.25G/9.98G [00:20<00:06, 396MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73% 7.30G/9.98G [00:20<00:06, 401MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74% 7.34G/9.98G [00:20<00:06, 380MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74% 7.38G/9.98G [00:21<00:06, 376MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75% 7.43G/9.98G [00:21<00:06, 392MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75% 7.48G/9.98G [00:21<00:06, 381MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75% 7.52G/9.98G [00:21<00:06, 391MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76% 7.56G/9.98G [00:21<00:06, 399MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76% 7.61G/9.98G [00:21<00:05, 409MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77% 7.67G/9.98G [00:21<00:05, 412MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77% 7.72G/9.98G [00:21<00:05, 420MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78% 7.77G/9.98G [00:22<00:05, 426MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78% 7.82G/9.98G [00:22<00:05, 428MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79% 7.87G/9.98G [00:22<00:04, 431MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79% 7.93G/9.98G [00:22<00:04, 416MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80% 7.97G/9.98G [00:22<00:05, 387MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80% 8.01G/9.98G [00:22<00:05, 335MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81% 8.06G/9.98G [00:22<00:05, 363MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81% 8.12G/9.98G [00:22<00:04, 399MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82% 8.16G/9.98G [00:23<00:04, 402MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82% 8.20G/9.98G [00:23<00:04, 397MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83% 8.24G/9.98G [00:23<00:04, 400MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83% 8.28G/9.98G [00:23<00:04, 378MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83% 8.33G/9.98G [00:23<00:04, 370MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84% 8.37G/9.98G [00:23<00:04, 373MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84% 8.42G/9.98G [00:23<00:03, 390MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85% 8.47G/9.98G [00:23<00:03, 405MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85% 8.51G/9.98G [00:24<00:04, 305MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86% 8.56G/9.98G [00:24<00:04, 297MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86% 8.61G/9.98G [00:24<00:04, 339MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87% 8.66G/9.98G [00:24<00:03, 375MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87% 8.71G/9.98G [00:24<00:03, 404MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88% 8.77G/9.98G [00:24<00:02, 426MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88% 8.82G/9.98G [00:24<00:02, 388MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89% 8.87G/9.98G [00:24<00:02, 400MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89% 8.91G/9.98G [00:25<00:02, 386MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90% 8.97G/9.98G [00:25<00:02, 401MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90% 9.02G/9.98G [00:25<00:02, 419MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91% 9.07G/9.98G [00:25<00:02, 417MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91% 9.12G/9.98G [00:25<00:02, 413MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92% 9.18G/9.98G [00:25<00:01, 428MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92% 9.23G/9.98G [00:25<00:01, 424MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93% 9.28G/9.98G [00:26<00:02, 344MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93% 9.32G/9.98G [00:26<00:01, 337MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94% 9.36G/9.98G [00:26<00:01, 341MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94% 9.42G/9.98G [00:26<00:01, 365MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95% 9.46G/9.98G [00:26<00:01, 371MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95% 9.50G/9.98G [00:26<00:01, 374MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96% 9.55G/9.98G [00:26<00:01, 390MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96% 9.59G/9.98G [00:26<00:00, 383MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97% 9.64G/9.98G [00:26<00:00, 369MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97% 9.68G/9.98G [00:27<00:00, 375MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97% 9.72G/9.98G [00:27<00:00, 376MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98% 9.76G/9.98G [00:27<00:00, 363MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98% 9.80G/9.98G [00:27<00:00, 371MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99% 9.85G/9.98G [00:27<00:00, 345MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99% 9.89G/9.98G [00:27<00:00, 338MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100% 9.93G/9.98G [00:27<00:00, 332MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100% 9.98G/9.98G [00:27<00:00, 356MB/s]\n",
      "Downloading shards:  50% 1/2 [00:28<00:28, 28.26s/it]\n",
      "model-00002-of-00002.safetensors:   0% 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   1% 41.9M/3.50G [00:00<00:09, 381MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   2% 83.9M/3.50G [00:00<00:12, 283MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   3% 115M/3.50G [00:00<00:11, 287MB/s] \u001b[A\n",
      "model-00002-of-00002.safetensors:   4% 147M/3.50G [00:00<00:12, 275MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   5% 178M/3.50G [00:00<00:14, 236MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   6% 210M/3.50G [00:00<00:15, 207MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7% 241M/3.50G [00:00<00:14, 223MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   8% 273M/3.50G [00:01<00:14, 224MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   9% 304M/3.50G [00:01<00:14, 224MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10% 346M/3.50G [00:01<00:12, 260MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  11% 398M/3.50G [00:01<00:10, 310MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  13% 451M/3.50G [00:01<00:08, 348MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14% 493M/3.50G [00:01<00:08, 338MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  15% 535M/3.50G [00:01<00:08, 338MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  16% 577M/3.50G [00:02<00:08, 334MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  18% 619M/3.50G [00:02<00:09, 316MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  19% 661M/3.50G [00:02<00:09, 310MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  20% 703M/3.50G [00:02<00:08, 328MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  21% 744M/3.50G [00:02<00:07, 349MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  22% 786M/3.50G [00:02<00:07, 348MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  24% 828M/3.50G [00:02<00:11, 240MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  25% 881M/3.50G [00:03<00:09, 289MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  26% 923M/3.50G [00:03<00:08, 308MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  28% 965M/3.50G [00:03<00:07, 325MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  29% 1.01G/3.50G [00:03<00:07, 344MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  30% 1.05G/3.50G [00:03<00:07, 335MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  31% 1.09G/3.50G [00:03<00:07, 332MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  32% 1.13G/3.50G [00:03<00:07, 330MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  34% 1.17G/3.50G [00:03<00:07, 303MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  35% 1.23G/3.50G [00:04<00:06, 329MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  36% 1.27G/3.50G [00:04<00:07, 298MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  37% 1.31G/3.50G [00:04<00:07, 308MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  39% 1.35G/3.50G [00:04<00:07, 300MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  40% 1.38G/3.50G [00:04<00:07, 302MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  41% 1.44G/3.50G [00:04<00:06, 343MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  42% 1.48G/3.50G [00:04<00:05, 350MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  44% 1.53G/3.50G [00:04<00:05, 358MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  45% 1.57G/3.50G [00:05<00:05, 353MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  46% 1.61G/3.50G [00:05<00:05, 350MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  47% 1.66G/3.50G [00:05<00:05, 363MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  49% 1.70G/3.50G [00:05<00:05, 354MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  50% 1.74G/3.50G [00:05<00:04, 371MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  51% 1.78G/3.50G [00:05<00:04, 363MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  52% 1.82G/3.50G [00:05<00:04, 360MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  53% 1.87G/3.50G [00:05<00:04, 343MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  55% 1.91G/3.50G [00:06<00:04, 341MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  56% 1.95G/3.50G [00:06<00:04, 346MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  57% 1.99G/3.50G [00:06<00:04, 342MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  58% 2.03G/3.50G [00:06<00:04, 323MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  59% 2.08G/3.50G [00:06<00:04, 339MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  61% 2.13G/3.50G [00:06<00:03, 378MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  62% 2.18G/3.50G [00:06<00:03, 409MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  64% 2.23G/3.50G [00:06<00:02, 428MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  65% 2.29G/3.50G [00:07<00:03, 383MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  67% 2.33G/3.50G [00:07<00:03, 333MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  68% 2.37G/3.50G [00:07<00:03, 308MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  69% 2.42G/3.50G [00:07<00:03, 337MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  70% 2.46G/3.50G [00:07<00:02, 349MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  72% 2.51G/3.50G [00:07<00:02, 354MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  73% 2.55G/3.50G [00:07<00:02, 364MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  74% 2.59G/3.50G [00:07<00:02, 360MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  75% 2.63G/3.50G [00:08<00:02, 375MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  76% 2.67G/3.50G [00:08<00:02, 323MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  78% 2.72G/3.50G [00:08<00:02, 339MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  79% 2.76G/3.50G [00:08<00:02, 345MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  80% 2.80G/3.50G [00:08<00:01, 351MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  81% 2.84G/3.50G [00:08<00:01, 367MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  83% 2.89G/3.50G [00:08<00:01, 384MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  84% 2.94G/3.50G [00:08<00:01, 393MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  85% 2.99G/3.50G [00:09<00:01, 393MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  87% 3.03G/3.50G [00:09<00:01, 382MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  88% 3.07G/3.50G [00:09<00:01, 371MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  89% 3.11G/3.50G [00:09<00:01, 350MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  90% 3.16G/3.50G [00:09<00:00, 349MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  91% 3.20G/3.50G [00:09<00:01, 282MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  93% 3.24G/3.50G [00:09<00:00, 299MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  94% 3.28G/3.50G [00:09<00:00, 317MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  95% 3.32G/3.50G [00:10<00:00, 319MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  96% 3.37G/3.50G [00:10<00:00, 326MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  97% 3.41G/3.50G [00:10<00:00, 329MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  99% 3.45G/3.50G [00:10<00:00, 324MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors: 100% 3.50G/3.50G [00:10<00:00, 328MB/s]\n",
      "Downloading shards: 100% 2/2 [00:39<00:00, 19.59s/it]\n",
      "Loading checkpoint shards: 100% 2/2 [00:09<00:00,  4.77s/it]\n",
      "generation_config.json: 100% 188/188 [00:00<00:00, 1.63MB/s]\n",
      "Llama-2-7b-chat-hf\n",
      "Configuring for Llama2...\n",
      "Using 1/1 GPUs - 13.48 GB allocated per GPU\n",
      "Loading prompts from txt file\n",
      "['[INST] <<SYS>> You are a helpful assistant. Always respond truthfully and to the best of your ability. <</SYS>> Tell me about Deleuze concept of ritournelle\\n [/INST]', '[INST] <<SYS>> You are a helpful assistant. Always respond truthfully and to the best of your ability. <</SYS>> Give me a list of the most important non-European philosphers of the twentieth century\\n [/INST]', \"[INST] <<SYS>> You are a helpful assistant. Always respond truthfully and to the best of your ability. <</SYS>> What was Wikipedia's impact on knowledge sharing? [/INST]\"]\n",
      "Saving generation in:  results/interactive_prompting/Llama-2-7b-chat-hf/results_815_sentence-wm_2_2_256_4_1.0.jsonl\n",
      "Saving detection in:  results/interactive_prompting/Llama-2-7b-chat-hf/scores_815_sentence-wm_2_2_256_4_1.0.jsonl\n",
      "Sampling mode\n",
      "Beam Search: None\n",
      "Beam chunk size: 0\n",
      "Starting from 3\n",
      "/content/drive/MyDrive/NLP Projects/WaterMax/src/watermax.py:113: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(f\"Average time per prompt: {np.sum(all_times) / (len(prompts) - start_point) :.2f}\")\n",
      "Average time per prompt: nan\n",
      "/content/drive/MyDrive/NLP Projects/WaterMax/src/watermax.py:116: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(f\"Average time per prompt: {np.sum(all_times) / (len(prompts) - start_point) :.2f}\")\n",
      "Average time per prompt: nan\n",
      "  0% 0/3 [00:00<?, ?it/s]/content/drive/MyDrive/NLP Projects/WaterMax/src/models/wm.py:1152: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  seed = int(np.mod(seed * salt + np.int64(i), np.int64(2**63 - 1)))\n",
      "100% 3/3 [00:00<00:00,  3.49it/s]\n",
      ">>> Scores: \n",
      "       text_index  num_token     score    pvalue  all_pvalues  log10_pvalue\n",
      "count         3.0        3.0  3.000000  3.000000     3.000000      3.000000\n",
      "mean          1.0      256.0  1.827528  0.087792     0.087792     -1.097852\n",
      "std           1.0        0.0  0.298400  0.045015     0.045015      0.236872\n",
      "min           0.0      256.0  1.633991  0.045464     0.045464     -1.342332\n",
      "50%           1.0      256.0  1.677416  0.082829     0.082829     -1.081820\n",
      "max           2.0      256.0  2.171178  0.135082     0.135082     -0.869403\n",
      "Saved scores to results/interactive_prompting/Llama-2-7b-chat-hf/scores_815_sentence-wm_2_2_256_4_1.0.jsonl\n"
     ]
    }
   ],
   "source": [
    "!python src/watermax.py --model_name meta-llama/Llama-2-7b-chat-hf --generate --detect --seed=815 --ngram=4 --n=2 --N=2 --prompts data/test_prompts.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daae8ab",
   "metadata": {},
   "source": [
    "Following function outputs the results and scores from the JSON files generated using the Watermax model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7ef2a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def process_json_file(filepath, max_lines = 3):\n",
    "\n",
    "    count = 0\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                if count >= max_lines:\n",
    "                    break\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    print(data)\n",
    "                    count += 1\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON: {e}\")\n",
    "                    print(f\"Problematic line: {line}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "YKz_lO0XK4Ny",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1034,
     "status": "ok",
     "timestamp": 1744566645686,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "YKz_lO0XK4Ny",
    "outputId": "7f0c05eb-5ea2-415e-f544-6dc4a97511a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '[INST] <<SYS>> You are a helpful assistant. Always respond truthfully and to the best of your ability. <</SYS>> Tell me about Deleuze concept of ritournelle\\n [/INST]', 'result': '  Gilles Deleuze was a French philosopher known for his complex and abstract ideas. One concept that he wrote about is \"ritournelle,\" which is French for \"round trip\" or \"return journey.\" In Deleuze\\'s philosophy, ritournelle refers to the process of repetition and return, which he believed was fundamental to the creation of new ideas and the transformation of societies.\\n\\nAccording to Deleuze, ritournelle is a concept that is closely related to his ideas about difference and repetition. In his view, repetition is not simply a matter of repeating something that has already been said or done, but rather it involves the creation of new connections and relationships between different elements. When we repeat something, we are not simply reiterating the same old idea or action, but rather we are creating a new network of relationships that can lead to new insights and innovations.\\n\\nFor Deleuze, the process of ritournelle involves the continuous cycling of ideas and practices through a series of transformations and reinterpretations. This cycle of repetition and return is not simply a mechanical process, but rather it involves the creative intervention of individuals and groups who are constantly', 'speed': 0.01132249052096294, 'eta': '00h04m24s'}\n",
      "{'prompt': '[INST] <<SYS>> You are a helpful assistant. Always respond truthfully and to the best of your ability. <</SYS>> Give me a list of the most important non-European philosphers of the twentieth century\\n [/INST]', 'result': \"  Certainly! Here are some of the most important non-European philosophers of the twentieth century:\\n\\n1. Ali Shariati (1933-1977) - Iranian philosopher and Islamic scholar, known for his interpretations of Islamic philosophy and his influence on Islamic fundamentalism.\\n2. Frantz Fanon (1925-1961) - French philosopher and psychiatrist from Martinique, known for his work on colonialism, racism, and the psychological effects of oppression.\\n3. Mahatma Gandhi (1869-1948) - Indian political leader and philosopher, known for his advocacy of nonviolent civil disobedience and his role in India's struggle for independence from British colonial rule.\\n4. Alain Locke (1885-1954) - American philosopher and educator, known for his work on race and identity in the United States, and his advocacy of the Harlem Renaissance movement.\\n5. Trinidadian philosopher, Edward Kamau Brathwaite (193\", 'speed': 0.010589108748364247, 'eta': '00h03m08s'}\n",
      "{'prompt': \"[INST] <<SYS>> You are a helpful assistant. Always respond truthfully and to the best of your ability. <</SYS>> What was Wikipedia's impact on knowledge sharing? [/INST]\", 'result': \"  Wikipedia has had a significant impact on knowledge sharing since its launch in 2001. Here are some of the ways in which Wikipedia has influenced the dissemination of knowledge:\\n\\n1. Open Access: Wikipedia is a free online encyclopedia that makes knowledge accessible to people all over the world. Anyone with an internet connection can access and contribute to the platform, regardless of their location or socio-economic status.\\n2. Collaborative Learning: Wikipedia operates on the principle of collaborative learning, where multiple editors work together to create and edit articles. This has led to a vast repository of knowledge that is constantly updated and improved upon by a diverse group of contributors.\\n3. democratization of Knowledge: By removing the barriers of access to knowledge, Wikipedia has democratized knowledge sharing. Anyone can contribute to the platform, regardless of their background or credentials, which has led to a more inclusive and diverse knowledge base.\\n4. Acceleration of Knowledge Sharing: Wikipedia's platform has accelerated the sharing of knowledge by providing a central hub for information. This has enabled people to access and share knowledge more quickly and easily than ever before.\\n5. Diffusion of Information\", 'speed': 0.08366071643528097, 'eta': '00h00m11s'}\n"
     ]
    }
   ],
   "source": [
    "filepath = \"results/interactive_prompting/Llama-2-7b-chat-hf/results_815_sentence-wm_2_2_256_4_1.0.jsonl\"\n",
    "\n",
    "process_json_file(filepath, max_lines=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "LVGwfRqUK3_I",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 639,
     "status": "ok",
     "timestamp": 1744566648867,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "LVGwfRqUK3_I",
    "outputId": "dc189d79-244d-4d27-f486-f01eed88423d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_index': 0, 'num_token': 256, 'score': 1.6774163175457097, 'pvalue': 0.1350818995012488, 'all_pvalues': 0.1350818995012488}\n",
      "{'text_index': 1, 'num_token': 256, 'score': 1.6339906471156715, 'pvalue': 0.08282862079932456, 'all_pvalues': 0.08282862079932456}\n",
      "{'text_index': 2, 'num_token': 256, 'score': 2.1711781382904274, 'pvalue': 0.04546402425198288, 'all_pvalues': 0.04546402425198288}\n"
     ]
    }
   ],
   "source": [
    "filepath = \"results/interactive_prompting/Llama-2-7b-chat-hf/scores_815_sentence-wm_2_2_256_4_1.0.jsonl\"\n",
    "\n",
    "process_json_file(filepath, max_lines=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253bae77",
   "metadata": {},
   "source": [
    "Prompts can directly be passed through the command which is demonstrated in these cells along with the output results and their relevant scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kr_VCZM213ME",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17345,
     "status": "ok",
     "timestamp": 1744514732960,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "kr_VCZM213ME",
    "outputId": "8e8b7deb-7876-4961-8ca3-1db9e337c972"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-13 03:25:21.267582: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-13 03:25:21.296666: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744514721.323340    6157 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744514721.331630    6157 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-13 03:25:21.362200: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Loading checkpoint shards: 100% 2/2 [00:05<00:00,  2.55s/it]\n",
      "Llama-2-7b-chat-hf\n",
      "Configuring for Llama2...\n",
      "Using 1/1 GPUs - 13.48 GB allocated per GPU\n",
      "Prompt:  [\"[INST] <<SYS>> You are a helpful assistant. Always respond truthfully and to the best of your ability. <</SYS>> What was Spinoza's relationship with Leibniz? [/INST]\", '[INST] <<SYS>> You are a helpful assistant. Always respond truthfully and to the best of your ability. <</SYS>> Which philospher spoke about the multicolored cow? [/INST]']\n",
      "Saving generation in:  results/interactive_prompting/Llama-2-7b-chat-hf/results_926_sentence-wm_2_2_256_6_1.0.jsonl\n",
      "Saving detection in:  results/interactive_prompting/Llama-2-7b-chat-hf/scores_926_sentence-wm_2_2_256_6_1.0.jsonl\n",
      "Sampling mode\n",
      "Beam Search: None\n",
      "Beam chunk size: 0\n",
      "Starting from 2\n",
      "/content/drive/MyDrive/NLP Projects/WaterMax/src/watermax.py:113: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(f\"Average time per prompt: {np.sum(all_times) / (len(prompts) - start_point) :.2f}\")\n",
      "Average time per prompt: nan\n",
      "/content/drive/MyDrive/NLP Projects/WaterMax/src/watermax.py:116: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(f\"Average time per prompt: {np.sum(all_times) / (len(prompts) - start_point) :.2f}\")\n",
      "Average time per prompt: nan\n",
      "  0% 0/2 [00:00<?, ?it/s]/content/drive/MyDrive/NLP Projects/WaterMax/src/models/wm.py:1152: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  seed = int(np.mod(seed * salt + np.int64(i), np.int64(2**63 - 1)))\n",
      "100% 2/2 [00:00<00:00, 68.93it/s]\n",
      ">>> Scores: \n",
      "       text_index   num_token     score    pvalue  all_pvalues  log10_pvalue\n",
      "count    2.000000    2.000000  2.000000  2.000000     2.000000      2.000000\n",
      "mean     0.500000  195.000000  0.262500  0.418061     0.418061     -0.538953\n",
      "std      0.707107   86.267027  1.590438  0.427076     0.427076      0.560475\n",
      "min      0.000000  134.000000 -0.862109  0.116073     0.116073     -0.935269\n",
      "50%      0.500000  195.000000  0.262500  0.418061     0.418061     -0.538953\n",
      "max      1.000000  256.000000  1.387110  0.720049     0.720049     -0.142638\n",
      "Saved scores to results/interactive_prompting/Llama-2-7b-chat-hf/scores_926_sentence-wm_2_2_256_6_1.0.jsonl\n"
     ]
    }
   ],
   "source": [
    "!python src/watermax.py --model_name meta-llama/Llama-2-7b-chat-hf --generate --detect --seed=926 --ngram=6 --n=2 --N=2 --fp16  --prompts \"What was Spinoza's relationship with Leibniz?\" \"Which philospher spoke about the multicolored cow?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "zZTFZz8xLiP8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1744514773011,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "zZTFZz8xLiP8",
    "outputId": "3c17416b-df4e-4d14-e12f-9a836c183d8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': \"[INST] <<SYS>> You are a helpful assistant. Always respond truthfully and to the best of your ability. <</SYS>> What was Spinoza's relationship with Leibniz? [/INST]\", 'result': '  Baruch Spinoza and Gottfried Wilhelm Leibniz were two influential philosophers who lived and worked in the 17th century, during the Dutch Golden Age of Philosophy. While they were both contemporary philosophers and shared some common interests and ideas, their relationship was somewhat complex and contentious.\\n\\nLeibniz, who was born in 1646, was a German philosopher and mathematician who is best known for his work on calculus and his theory of monads. Spinoza, who was born in 1632, was a Dutch philosopher who is best known for his ethical and political philosophy, particularly his theories on substance monism, causality, and the nature of God.\\n\\nDespite their differences in background and philosophy, Leibniz and Spinoza had some contact and interaction with each other. According to Spinoza\\'s biographer, Steven Blumann, Leibniz sent Spinoza a copy of his work \"De monadibus\" (On Monads) in 1676, and Spinoza replied with a critique of Leibniz\\'s ideas. However, there is no evidence', 'speed': 0.09556391817204624, 'eta': '00h00m20s'}\n",
      "{'prompt': '[INST] <<SYS>> You are a helpful assistant. Always respond truthfully and to the best of your ability. <</SYS>> Which philospher spoke about the multicolored cow? [/INST]', 'result': '  I apologize, but there is no philosopher who has spoken about a \"multicolored cow.\" The concept of a multicolored cow is a fictional or metaphorical idea, and there is no real philosophical discussion or theory surrounding it.\\n\\nPhilosophers have written extensively on various topics such as ethics, metaphysics, epistemology, politics, and aesthetics, among others. However, there is no record of any philosopher discussing the multicolored cow.\\n\\nIf you have any other questions or topics you would like to discuss, please feel free to ask!', 'speed': 0.19587162064219565, 'eta': '00h00m05s'}\n",
      "{'prompt': '[INST] <<SYS>> You are a helpful assistant. Always respond truthfully and to the best of your ability. <</SYS>> Marketers are employed in the public sector as well as the private sector.\\\\nMost people think of marketing as a strictly private sector activity, but the reality is people with sales and marketing backgrounds are hired by government agencies in a number of capacities. Government agencies at the local, state and federal level all employ marketing professionals in areas including, but not limited to, public relations, property disposal, bond sales and purchasing.\\\\nAlmost all major government agencies have their own public-relations staff, and in many cases it is a stand-alone department with a public relations or media director and several support staff. Government agency PR departments are responsible for producing news releases, holding press conferences, and generally promoting activities of the agency, such as tourism or encouraging new businesses to move into the area.\\\\nGovernment agencies are constantly buying supplies, equipment and other property and selling off old equipment and property. The departments tasked with disposing of this government property often hire individuals with a background in marketing. Their job is to assist the agency in coming up with creative ways to sell or otherwise dispose of obsolete government property.\\\\n\", \"baseline\": \"Most government agencies have to follow complicated regulations for purchasing supplies and equipment. In many cases, agencies can only buy from certain suppliers or can only buy goods within specific price ranges. Agencies often hire purchasing agents with marketing backgrounds to help them manage their purchases, including setting criteria for suppliers and establishing price ranges. Government purchasing agents must be very familiar with current purchasing regulations at their agency.\\\\nLocal, state and federal agencies sometimes sell bonds to finance capital improvement projects in their jurisdictions. These agencies hire bond and securities marketing specialists to prepare bond prospectuses for investors, which detail the terms of the bond, and to organize marketing campaigns to sell the bonds. Bond marketing specialists also typically manage media relations for bond campaigns.\\\\nBrowne, Clayton. \\\\\"Are There Marketing Jobs in the Government?\\\\\" Work - Chron.com, http://work.chron.com/there-marketing-jobs-government-22921.html. Accessed 24 April 2019.\\\\nWhat Is the Role of the Owner of the Supermarket?\\\\nWhat Does a Beer Rep Do?\\n [/INST]', 'result': '  As a helpful assistant, I must inform you that you have provided a mix of accurate and outdated information in your question. Here are some corrections and updates to help you better understand the role of marketers in the public and private sectors:\\n\\n1. Public sector marketing: You are correct that government agencies employ marketers in various capacities. However, the role of marketers in the public sector has evolved significantly over the years. While some government agencies still have standalone public relations departments, many have integrated marketing and communications functions to better serve their constituents. Marketers in the public sector are responsible for creating and implementing communication strategies to raise awareness about government programs, services, and initiatives.\\n2. Private sector marketing: You are also correct that marketers are employed in the private sector, but the definition of marketing has expanded beyond traditional advertising and sales promotion. Modern marketers in the private sector are responsible for creating and executing comprehensive marketing strategies that encompass multiple channels and touchpoints, including digital marketing, social media, content marketing, and customer experience.\\n3. Bond marketing: You mentioned that government agencies hire bond and secur', 'speed': 0.08335848580447659, 'eta': '02h17m57s'}\n"
     ]
    }
   ],
   "source": [
    "filepath = \"results/interactive_prompting/Llama-2-7b-chat-hf/results_926_sentence-wm_2_2_256_6_1.0.jsonl\"\n",
    "\n",
    "process_json_file(filepath, max_lines=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0r4DMEC9LkeW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1744514774905,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "0r4DMEC9LkeW",
    "outputId": "5bc5c2bc-b48e-4747-a957-05e042d1abbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_index': 0, 'num_token': 256, 'score': 1.3871101265094385, 'pvalue': 0.11607291506583266, 'all_pvalues': 0.11607291506583266}\n",
      "{'text_index': 1, 'num_token': 134, 'score': -0.8621091576761015, 'pvalue': 0.7200491884683103, 'all_pvalues': 0.7200491884683103}\n"
     ]
    }
   ],
   "source": [
    "filepath = \"results/interactive_prompting/Llama-2-7b-chat-hf/scores_926_sentence-wm_2_2_256_6_1.0.jsonl\"\n",
    "\n",
    "process_json_file(filepath, max_lines=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae7d01f",
   "metadata": {},
   "source": [
    "Using the following command we are generating the benchmarks for Watermax. The test_sentence_wm.py script allows to perform 6 different operations:\n",
    "\n",
    "- **generate**: Performs the watermarking operation specified by mode with parameters param1 and param2 for prompts defined by benches. Outputs a json file containing the generated texts.\n",
    "- **detect**: Performs watermark detection using the method specified by mode with parameters param1 and param2 for results of benches. Outputs a json file containing the p-value of each text.\n",
    "- **rate**: Use the model specified by model_name to rate the resulting texts of benches\n",
    "- **standardize** (MMW only): Standardize the results and score files to be compatible with the Mark My Words benchmark\n",
    "- **detect_std** (MMW only): Performs detection on texts generated using Mark My Words, usually used to compute the p-value of texts attacked using the MMW attack suite\n",
    "- **standardize_final** (MMW only): Same as standardize but expects a file containing the detection results from detect_std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C_6dl3WgLwLp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9089117,
     "status": "ok",
     "timestamp": 1744524977863,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "C_6dl3WgLwLp",
    "outputId": "9db09c83-c2b5-45f5-851d-c0cfdcb2e637"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-13 03:44:54.211191: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-13 03:44:54.240690: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744515894.267331   11288 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744515894.276412   11288 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-13 03:44:54.306760: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Loading checkpoint shards: 100% 2/2 [00:09<00:00,  4.53s/it]\n",
      "Llama-2-7b-chat-hf\n",
      "Configuring for Llama2...\n",
      "Using 1/1 GPUs - 13.48 GB allocated per GPU\n",
      "Benchmarks:  ['story_reports', 'fake_news', 'invented_stories']\n",
      "Saving generation in:  results/benchmark/Llama-2-7b-chat-hf/results_story_reports_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\n",
      "Saving detection in:  results/benchmark/Llama-2-7b-chat-hf/scores_story_reports_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\n",
      "Saving final detection in:  results/benchmark/Llama-2-7b-chat-hf/final_scores_story_reports_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\n",
      "Nucleus:  0.95\n",
      "Sampling mode\n",
      "Beam Search: None\n",
      "Beam chunk size: 6\n",
      "Starting from 2\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/content/drive/MyDrive/NLP Projects/WaterMax/src/models/wm.py:1350: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  seed = int(np.mod(seed * salt + np.int64(i), np.int64(2**63 - 1)))\n",
      "Generated     2 -     3 - Speed 0.03 prompts/s - ETA 01h03m06s\n",
      "Generated     3 -     4 - Speed 0.03 prompts/s - ETA 01h01m43s\n",
      "Generated     4 -     5 - Speed 0.02 prompts/s - ETA 01h04m35s\n",
      "Generated     5 -     6 - Speed 0.03 prompts/s - ETA 00h48m58s\n",
      "Generated     6 -     7 - Speed 0.03 prompts/s - ETA 01h02m17s\n",
      "Generated     7 -     8 - Speed 0.03 prompts/s - ETA 01h01m42s\n",
      "Generated     8 -     9 - Speed 0.02 prompts/s - ETA 01h01m45s\n",
      "Generated     9 -    10 - Speed 0.03 prompts/s - ETA 00h49m06s\n",
      "Generated    10 -    11 - Speed 0.02 prompts/s - ETA 01h00m24s\n",
      "Generated    11 -    12 - Speed 0.03 prompts/s - ETA 00h59m06s\n",
      "Generated    12 -    13 - Speed 0.03 prompts/s - ETA 00h52m26s\n",
      "Generated    13 -    14 - Speed 0.02 prompts/s - ETA 00h58m25s\n",
      "Generated    14 -    15 - Speed 0.03 prompts/s - ETA 00h57m02s\n",
      "Generated    15 -    16 - Speed 0.03 prompts/s - ETA 00h49m05s\n",
      "Generated    16 -    17 - Speed 0.03 prompts/s - ETA 00h43m42s\n",
      "Generated    17 -    18 - Speed 0.02 prompts/s - ETA 00h55m57s\n",
      "Generated    18 -    19 - Speed 0.02 prompts/s - ETA 00h55m25s\n",
      "Generated    19 -    20 - Speed 0.03 prompts/s - ETA 00h51m17s\n",
      "Generated    20 -    21 - Speed 0.02 prompts/s - ETA 00h53m57s\n",
      "Generated    21 -    22 - Speed 0.02 prompts/s - ETA 00h53m00s\n",
      "Generated    22 -    23 - Speed 0.03 prompts/s - ETA 00h49m38s\n",
      "Generated    23 -    24 - Speed 0.03 prompts/s - ETA 00h39m38s\n",
      "Generated    24 -    25 - Speed 0.03 prompts/s - ETA 00h50m20s\n",
      "Generated    25 -    26 - Speed 0.02 prompts/s - ETA 00h50m34s\n",
      "Generated    26 -    27 - Speed 0.02 prompts/s - ETA 00h50m06s\n",
      "Generated    27 -    28 - Speed 0.02 prompts/s - ETA 00h49m18s\n",
      "Generated    28 -    29 - Speed 0.02 prompts/s - ETA 00h48m23s\n",
      "Generated    29 -    30 - Speed 0.04 prompts/s - ETA 00h28m24s\n",
      "Generated    30 -    31 - Speed 0.03 prompts/s - ETA 00h44m20s\n",
      "Generated    31 -    32 - Speed 0.02 prompts/s - ETA 00h46m17s\n",
      "Generated    32 -    33 - Speed 0.03 prompts/s - ETA 00h32m36s\n",
      "Generated    33 -    34 - Speed 0.03 prompts/s - ETA 00h35m05s\n",
      "Generated    34 -    35 - Speed 0.03 prompts/s - ETA 00h43m35s\n",
      "Generated    35 -    36 - Speed 0.03 prompts/s - ETA 00h34m28s\n",
      "Generated    36 -    37 - Speed 0.03 prompts/s - ETA 00h40m05s\n",
      "Generated    37 -    38 - Speed 0.03 prompts/s - ETA 00h34m46s\n",
      "Generated    38 -    39 - Speed 0.03 prompts/s - ETA 00h33m52s\n",
      "Generated    39 -    40 - Speed 0.03 prompts/s - ETA 00h36m48s\n",
      "Generated    40 -    41 - Speed 0.02 prompts/s - ETA 00h40m06s\n",
      "Generated    41 -    42 - Speed 0.03 prompts/s - ETA 00h35m57s\n",
      "Generated    42 -    43 - Speed 0.02 prompts/s - ETA 00h38m49s\n",
      "Generated    43 -    44 - Speed 0.03 prompts/s - ETA 00h37m59s\n",
      "Generated    44 -    45 - Speed 0.03 prompts/s - ETA 00h33m20s\n",
      "Generated    45 -    46 - Speed 0.03 prompts/s - ETA 00h26m46s\n",
      "Generated    46 -    47 - Speed 0.03 prompts/s - ETA 00h35m26s\n",
      "Generated    47 -    48 - Speed 0.02 prompts/s - ETA 00h35m57s\n",
      "Generated    48 -    49 - Speed 0.03 prompts/s - ETA 00h26m28s\n",
      "Generated    49 -    50 - Speed 0.03 prompts/s - ETA 00h30m35s\n",
      "Generated    50 -    51 - Speed 0.02 prompts/s - ETA 00h33m49s\n",
      "Generated    51 -    52 - Speed 0.03 prompts/s - ETA 00h29m55s\n",
      "Generated    52 -    53 - Speed 0.02 prompts/s - ETA 00h32m22s\n",
      "Generated    53 -    54 - Speed 0.03 prompts/s - ETA 00h27m53s\n",
      "Generated    54 -    55 - Speed 0.03 prompts/s - ETA 00h29m44s\n",
      "Generated    55 -    56 - Speed 0.02 prompts/s - ETA 00h30m40s\n",
      "Generated    56 -    57 - Speed 0.02 prompts/s - ETA 00h29m40s\n",
      "Generated    57 -    58 - Speed 0.03 prompts/s - ETA 00h25m23s\n",
      "Generated    58 -    59 - Speed 0.03 prompts/s - ETA 00h23m50s\n",
      "Generated    59 -    60 - Speed 0.02 prompts/s - ETA 00h27m28s\n",
      "Generated    60 -    61 - Speed 0.02 prompts/s - ETA 00h26m59s\n",
      "Generated    61 -    62 - Speed 0.03 prompts/s - ETA 00h20m09s\n",
      "Generated    62 -    63 - Speed 0.02 prompts/s - ETA 00h25m22s\n",
      "Generated    63 -    64 - Speed 0.03 prompts/s - ETA 00h22m44s\n",
      "Generated    64 -    65 - Speed 0.03 prompts/s - ETA 00h23m58s\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generated    65 -    66 - Speed 0.04 prompts/s - ETA 00h13m33s\n",
      "Generated    66 -    67 - Speed 0.03 prompts/s - ETA 00h19m30s\n",
      "Generated    67 -    68 - Speed 0.02 prompts/s - ETA 00h22m03s\n",
      "Generated    68 -    69 - Speed 0.03 prompts/s - ETA 00h17m20s\n",
      "Generated    69 -    70 - Speed 0.03 prompts/s - ETA 00h18m07s\n",
      "Generated    70 -    71 - Speed 0.02 prompts/s - ETA 00h20m08s\n",
      "Generated    71 -    72 - Speed 0.04 prompts/s - ETA 00h11m13s\n",
      "Generated    72 -    73 - Speed 0.03 prompts/s - ETA 00h17m31s\n",
      "Generated    73 -    74 - Speed 0.03 prompts/s - ETA 00h16m12s\n",
      "Generated    74 -    75 - Speed 0.03 prompts/s - ETA 00h15m37s\n",
      "Generated    75 -    76 - Speed 0.02 prompts/s - ETA 00h16m43s\n",
      "Generated    76 -    77 - Speed 0.03 prompts/s - ETA 00h12m54s\n",
      "Generated    77 -    78 - Speed 0.02 prompts/s - ETA 00h15m20s\n",
      "Generated    78 -    79 - Speed 0.03 prompts/s - ETA 00h13m37s\n",
      "Generated    79 -    80 - Speed 0.03 prompts/s - ETA 00h12m12s\n",
      "Generated    80 -    81 - Speed 0.03 prompts/s - ETA 00h09m49s\n",
      "Generated    81 -    82 - Speed 0.03 prompts/s - ETA 00h12m37s\n",
      "Generated    82 -    83 - Speed 0.03 prompts/s - ETA 00h09m36s\n",
      "Generated    83 -    84 - Speed 0.03 prompts/s - ETA 00h09m56s\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generated    84 -    85 - Speed 0.02 prompts/s - ETA 00h10m41s\n",
      "Generated    85 -    86 - Speed 0.03 prompts/s - ETA 00h08m10s\n",
      "Generated    86 -    87 - Speed 0.04 prompts/s - ETA 00h06m33s\n",
      "Generated    87 -    88 - Speed 0.04 prompts/s - ETA 00h05m03s\n",
      "Generated    88 -    89 - Speed 0.02 prompts/s - ETA 00h08m00s\n",
      "Generated    89 -    90 - Speed 0.04 prompts/s - ETA 00h05m10s\n",
      "Generated    90 -    91 - Speed 0.03 prompts/s - ETA 00h05m15s\n",
      "Generated    91 -    92 - Speed 0.03 prompts/s - ETA 00h05m43s\n",
      "Generated    92 -    93 - Speed 0.02 prompts/s - ETA 00h05m23s\n",
      "Generated    93 -    94 - Speed 0.03 prompts/s - ETA 00h03m38s\n",
      "Generated    94 -    95 - Speed 0.02 prompts/s - ETA 00h04m01s\n",
      "Generated    95 -    96 - Speed 0.02 prompts/s - ETA 00h03m20s\n",
      "Generated    96 -    97 - Speed 0.03 prompts/s - ETA 00h02m10s\n",
      "Generated    97 -    98 - Speed 0.03 prompts/s - ETA 00h01m56s\n",
      "Generated    98 -    99 - Speed 0.02 prompts/s - ETA 00h01m22s\n",
      "Generated    99 -   100 - Speed 0.03 prompts/s - ETA 00h00m39s\n",
      "Average time per prompt: 36.45\n",
      "  0% 0/100 [00:00<?, ?it/s]/content/drive/MyDrive/NLP Projects/WaterMax/src/models/wm.py:1152: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  seed = int(np.mod(seed * salt + np.int64(i), np.int64(2**63 - 1)))\n",
      "100% 100/100 [00:05<00:00, 19.01it/s]\n",
      ">>> Scores: \n",
      "       text_index   num_token       score      pvalue  all_pvalues  log10_pvalue\n",
      "count  100.000000   100.00000  100.000000  100.000000   100.000000    100.000000\n",
      "mean    49.500000   883.80000    0.999159    0.281352     0.281352     -0.727189\n",
      "std     29.011492   139.82247    1.043159    0.208219     0.208219      0.498852\n",
      "min      0.000000   467.00000   -1.461070    0.000415     0.000415     -3.381987\n",
      "50%     49.500000   909.00000    0.995086    0.236761     0.236761     -0.625718\n",
      "max     99.000000  1024.00000    4.385722    0.842172     0.842172     -0.074599\n",
      "Saved scores to results/benchmark/Llama-2-7b-chat-hf/scores_story_reports_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\n",
      "Saving generation in:  results/benchmark/Llama-2-7b-chat-hf/results_fake_news_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\n",
      "Saving detection in:  results/benchmark/Llama-2-7b-chat-hf/scores_fake_news_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\n",
      "Saving final detection in:  results/benchmark/Llama-2-7b-chat-hf/final_scores_fake_news_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\n",
      "Nucleus:  0.95\n",
      "Sampling mode\n",
      "Beam Search: None\n",
      "Beam chunk size: 6\n",
      "Starting from 0\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/content/drive/MyDrive/NLP Projects/WaterMax/src/models/wm.py:1350: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  seed = int(np.mod(seed * salt + np.int64(i), np.int64(2**63 - 1)))\n",
      "Generated     0 -     1 - Speed 0.03 prompts/s - ETA 00h52m23s\n",
      "Generated     1 -     2 - Speed 0.04 prompts/s - ETA 00h45m29s\n",
      "Generated     2 -     3 - Speed 0.04 prompts/s - ETA 00h43m20s\n",
      "Generated     3 -     4 - Speed 0.04 prompts/s - ETA 00h40m51s\n",
      "Generated     4 -     5 - Speed 0.04 prompts/s - ETA 00h37m00s\n",
      "Generated     5 -     6 - Speed 0.04 prompts/s - ETA 00h39m48s\n",
      "Generated     6 -     7 - Speed 0.05 prompts/s - ETA 00h33m57s\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generated     7 -     8 - Speed 0.05 prompts/s - ETA 00h34m22s\n",
      "Generated     8 -     9 - Speed 0.04 prompts/s - ETA 00h42m25s\n",
      "Generated     9 -    10 - Speed 0.04 prompts/s - ETA 00h33m54s\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generated    10 -    11 - Speed 0.05 prompts/s - ETA 00h32m33s\n",
      "Generated    11 -    12 - Speed 0.05 prompts/s - ETA 00h31m19s\n",
      "Generated    12 -    13 - Speed 0.04 prompts/s - ETA 00h37m16s\n",
      "Generated    13 -    14 - Speed 0.04 prompts/s - ETA 00h38m59s\n",
      "Generated    14 -    15 - Speed 0.04 prompts/s - ETA 00h39m48s\n",
      "Generated    15 -    16 - Speed 0.03 prompts/s - ETA 00h41m42s\n",
      "Generated    16 -    17 - Speed 0.04 prompts/s - ETA 00h36m42s\n",
      "Generated    17 -    18 - Speed 0.04 prompts/s - ETA 00h39m06s\n",
      "Generated    18 -    19 - Speed 0.05 prompts/s - ETA 00h28m50s\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generated    19 -    20 - Speed 0.04 prompts/s - ETA 00h31m58s\n",
      "Generated    20 -    21 - Speed 0.03 prompts/s - ETA 00h39m43s\n",
      "Generated    21 -    22 - Speed 0.04 prompts/s - ETA 00h34m36s\n",
      "Generated    22 -    23 - Speed 0.03 prompts/s - ETA 00h38m59s\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generated    23 -    24 - Speed 0.03 prompts/s - ETA 00h42m14s\n",
      "Generated    24 -    25 - Speed 0.05 prompts/s - ETA 00h26m48s\n",
      "Generated    25 -    26 - Speed 0.04 prompts/s - ETA 00h30m25s\n",
      "Generated    26 -    27 - Speed 0.03 prompts/s - ETA 00h36m51s\n",
      "Generated    27 -    28 - Speed 0.04 prompts/s - ETA 00h29m49s\n",
      "Generated    28 -    29 - Speed 0.04 prompts/s - ETA 00h33m41s\n",
      "Generated    29 -    30 - Speed 0.04 prompts/s - ETA 00h30m55s\n",
      "Generated    30 -    31 - Speed 0.04 prompts/s - ETA 00h28m48s\n",
      "Generated    31 -    32 - Speed 0.04 prompts/s - ETA 00h26m45s\n",
      "Generated    32 -    33 - Speed 0.04 prompts/s - ETA 00h27m14s\n",
      "Generated    33 -    34 - Speed 0.04 prompts/s - ETA 00h28m24s\n",
      "Generated    34 -    35 - Speed 0.03 prompts/s - ETA 00h33m14s\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generated    35 -    36 - Speed 0.02 prompts/s - ETA 00h44m17s\n",
      "Generated    36 -    37 - Speed 0.04 prompts/s - ETA 00h29m03s\n",
      "Generated    37 -    38 - Speed 0.04 prompts/s - ETA 00h23m50s\n",
      "Generated    38 -    39 - Speed 0.04 prompts/s - ETA 00h28m19s\n",
      "Generated    39 -    40 - Speed 0.04 prompts/s - ETA 00h24m53s\n",
      "Generated    40 -    41 - Speed 0.04 prompts/s - ETA 00h22m19s\n",
      "Generated    41 -    42 - Speed 0.05 prompts/s - ETA 00h20m45s\n",
      "Generated    42 -    43 - Speed 0.04 prompts/s - ETA 00h21m29s\n",
      "Generated    43 -    44 - Speed 0.05 prompts/s - ETA 00h21m00s\n",
      "Generated    44 -    45 - Speed 0.03 prompts/s - ETA 00h29m16s\n",
      "Generated    45 -    46 - Speed 0.04 prompts/s - ETA 00h25m42s\n",
      "Generated    46 -    47 - Speed 0.04 prompts/s - ETA 00h22m06s\n",
      "Generated    47 -    48 - Speed 0.03 prompts/s - ETA 00h27m57s\n",
      "Generated    48 -    49 - Speed 0.04 prompts/s - ETA 00h20m12s\n",
      "Generated    49 -    50 - Speed 0.03 prompts/s - ETA 00h25m01s\n",
      "Generated    50 -    51 - Speed 0.03 prompts/s - ETA 00h24m22s\n",
      "Generated    51 -    52 - Speed 0.04 prompts/s - ETA 00h19m50s\n",
      "Generated    52 -    53 - Speed 0.04 prompts/s - ETA 00h19m18s\n",
      "Generated    53 -    54 - Speed 0.04 prompts/s - ETA 00h20m38s\n",
      "Generated    54 -    55 - Speed 0.04 prompts/s - ETA 00h19m11s\n",
      "Generated    55 -    56 - Speed 0.05 prompts/s - ETA 00h16m18s\n",
      "Generated    56 -    57 - Speed 0.05 prompts/s - ETA 00h15m35s\n",
      "Generated    57 -    58 - Speed 0.04 prompts/s - ETA 00h16m43s\n",
      "Generated    58 -    59 - Speed 0.04 prompts/s - ETA 00h16m40s\n",
      "Generated    59 -    60 - Speed 0.04 prompts/s - ETA 00h16m24s\n",
      "Generated    60 -    61 - Speed 0.04 prompts/s - ETA 00h17m06s\n",
      "Generated    61 -    62 - Speed 0.04 prompts/s - ETA 00h15m51s\n",
      "Generated    62 -    63 - Speed 0.04 prompts/s - ETA 00h14m35s\n",
      "Generated    63 -    64 - Speed 0.04 prompts/s - ETA 00h16m29s\n",
      "Generated    64 -    65 - Speed 0.03 prompts/s - ETA 00h18m02s\n",
      "Generated    65 -    66 - Speed 0.04 prompts/s - ETA 00h14m32s\n",
      "Generated    66 -    67 - Speed 0.03 prompts/s - ETA 00h19m28s\n",
      "Generated    67 -    68 - Speed 0.04 prompts/s - ETA 00h15m29s\n",
      "Generated    68 -    69 - Speed 0.03 prompts/s - ETA 00h15m25s\n",
      "Generated    69 -    70 - Speed 0.04 prompts/s - ETA 00h12m46s\n",
      "Generated    70 -    71 - Speed 0.04 prompts/s - ETA 00h12m21s\n",
      "Generated    71 -    72 - Speed 0.04 prompts/s - ETA 00h12m19s\n",
      "Generated    72 -    73 - Speed 0.04 prompts/s - ETA 00h11m33s\n",
      "Generated    73 -    74 - Speed 0.03 prompts/s - ETA 00h13m32s\n",
      "Generated    74 -    75 - Speed 0.03 prompts/s - ETA 00h13m54s\n",
      "Generated    75 -    76 - Speed 0.05 prompts/s - ETA 00h08m57s\n",
      "Generated    76 -    77 - Speed 0.03 prompts/s - ETA 00h12m42s\n",
      "Generated    77 -    78 - Speed 0.04 prompts/s - ETA 00h09m25s\n",
      "Generated    78 -    79 - Speed 0.03 prompts/s - ETA 00h10m32s\n",
      "Generated    79 -    80 - Speed 0.03 prompts/s - ETA 00h10m54s\n",
      "Generated    80 -    81 - Speed 0.04 prompts/s - ETA 00h07m48s\n",
      "Generated    81 -    82 - Speed 0.04 prompts/s - ETA 00h08m29s\n",
      "Generated    82 -    83 - Speed 0.04 prompts/s - ETA 00h07m25s\n",
      "Generated    83 -    84 - Speed 0.05 prompts/s - ETA 00h05m52s\n",
      "Generated    84 -    85 - Speed 0.04 prompts/s - ETA 00h07m35s\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generated    85 -    86 - Speed 0.03 prompts/s - ETA 00h09m09s\n",
      "Generated    86 -    87 - Speed 0.04 prompts/s - ETA 00h06m14s\n",
      "Generated    87 -    88 - Speed 0.04 prompts/s - ETA 00h05m54s\n",
      "Generated    88 -    89 - Speed 0.05 prompts/s - ETA 00h04m24s\n",
      "Generated    89 -    90 - Speed 0.04 prompts/s - ETA 00h04m06s\n",
      "Generated    90 -    91 - Speed 0.04 prompts/s - ETA 00h04m12s\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generated    91 -    92 - Speed 0.02 prompts/s - ETA 00h06m01s\n",
      "Generated    92 -    93 - Speed 0.03 prompts/s - ETA 00h04m03s\n",
      "Generated    93 -    94 - Speed 0.04 prompts/s - ETA 00h03m00s\n",
      "Generated    94 -    95 - Speed 0.03 prompts/s - ETA 00h03m08s\n",
      "Generated    95 -    96 - Speed 0.04 prompts/s - ETA 00h02m08s\n",
      "Generated    96 -    97 - Speed 0.04 prompts/s - ETA 00h01m35s\n",
      "Generated    97 -    98 - Speed 0.05 prompts/s - ETA 00h01m05s\n",
      "Generated    98 -    99 - Speed 0.04 prompts/s - ETA 00h00m55s\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generated    99 -   100 - Speed 0.03 prompts/s - ETA 00h00m29s\n",
      "Average time per prompt: 26.34\n",
      "  0% 0/100 [00:00<?, ?it/s]/content/drive/MyDrive/NLP Projects/WaterMax/src/models/wm.py:1152: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  seed = int(np.mod(seed * salt + np.int64(i), np.int64(2**63 - 1)))\n",
      "100% 100/100 [00:03<00:00, 27.15it/s]\n",
      ">>> Scores: \n",
      "       text_index    num_token       score      pvalue  all_pvalues  log10_pvalue\n",
      "count  100.000000   100.000000  100.000000  100.000000   100.000000    100.000000\n",
      "mean    49.500000   623.960000    1.043636    0.260771     0.260771     -0.773873\n",
      "std     29.011492    95.159613    1.157031    0.210956     0.210956      0.503894\n",
      "min      0.000000   450.000000   -2.836718    0.001593     0.001593     -2.797694\n",
      "50%     49.500000   616.000000    0.891630    0.218148     0.218148     -0.661250\n",
      "max     99.000000  1018.000000    3.886264    0.973114     0.973114     -0.011836\n",
      "Saved scores to results/benchmark/Llama-2-7b-chat-hf/scores_fake_news_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\n",
      "Saving generation in:  results/benchmark/Llama-2-7b-chat-hf/results_invented_stories_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\n",
      "Saving detection in:  results/benchmark/Llama-2-7b-chat-hf/scores_invented_stories_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\n",
      "Saving final detection in:  results/benchmark/Llama-2-7b-chat-hf/final_scores_invented_stories_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\n",
      "Nucleus:  0.95\n",
      "Sampling mode\n",
      "Beam Search: None\n",
      "Beam chunk size: 6\n",
      "Starting from 0\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/content/drive/MyDrive/NLP Projects/WaterMax/src/models/wm.py:1350: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  seed = int(np.mod(seed * salt + np.int64(i), np.int64(2**63 - 1)))\n",
      "Generated     0 -     1 - Speed 0.04 prompts/s - ETA 00h43m13s\n",
      "Generated     1 -     2 - Speed 0.04 prompts/s - ETA 00h44m59s\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generated     2 -     3 - Speed 0.05 prompts/s - ETA 00h33m20s\n",
      "Generated     3 -     4 - Speed 0.04 prompts/s - ETA 00h42m13s\n",
      "Generated     4 -     5 - Speed 0.04 prompts/s - ETA 00h41m49s\n",
      "Generated     5 -     6 - Speed 0.04 prompts/s - ETA 00h36m55s\n",
      "Generated     6 -     7 - Speed 0.02 prompts/s - ETA 01h00m29s\n",
      "Generated     7 -     8 - Speed 0.04 prompts/s - ETA 00h35m55s\n",
      "Generated     8 -     9 - Speed 0.04 prompts/s - ETA 00h41m22s\n",
      "Generated     9 -    10 - Speed 0.04 prompts/s - ETA 00h39m13s\n",
      "Generated    10 -    11 - Speed 0.03 prompts/s - ETA 00h57m08s\n",
      "Generated    11 -    12 - Speed 0.03 prompts/s - ETA 00h42m31s\n",
      "Generated    12 -    13 - Speed 0.03 prompts/s - ETA 00h47m40s\n",
      "Generated    13 -    14 - Speed 0.04 prompts/s - ETA 00h35m23s\n",
      "Generated    14 -    15 - Speed 0.03 prompts/s - ETA 00h49m30s\n",
      "Generated    15 -    16 - Speed 0.04 prompts/s - ETA 00h35m57s\n",
      "Generated    16 -    17 - Speed 0.03 prompts/s - ETA 00h41m46s\n",
      "Generated    17 -    18 - Speed 0.04 prompts/s - ETA 00h31m39s\n",
      "Generated    18 -    19 - Speed 0.03 prompts/s - ETA 00h44m01s\n",
      "Generated    19 -    20 - Speed 0.04 prompts/s - ETA 00h36m05s\n",
      "Generated    20 -    21 - Speed 0.03 prompts/s - ETA 00h37m17s\n",
      "Generated    21 -    22 - Speed 0.04 prompts/s - ETA 00h29m44s\n",
      "Generated    22 -    23 - Speed 0.03 prompts/s - ETA 00h46m25s\n",
      "Generated    23 -    24 - Speed 0.04 prompts/s - ETA 00h29m20s\n",
      "Generated    24 -    25 - Speed 0.04 prompts/s - ETA 00h28m26s\n",
      "Generated    25 -    26 - Speed 0.04 prompts/s - ETA 00h33m06s\n",
      "Generated    26 -    27 - Speed 0.02 prompts/s - ETA 00h46m58s\n",
      "Generated    27 -    28 - Speed 0.04 prompts/s - ETA 00h28m21s\n",
      "Generated    28 -    29 - Speed 0.04 prompts/s - ETA 00h31m33s\n",
      "Generated    29 -    30 - Speed 0.04 prompts/s - ETA 00h30m38s\n",
      "Generated    30 -    31 - Speed 0.04 prompts/s - ETA 00h24m49s\n",
      "Generated    31 -    32 - Speed 0.04 prompts/s - ETA 00h24m57s\n",
      "Generated    32 -    33 - Speed 0.03 prompts/s - ETA 00h31m42s\n",
      "Generated    33 -    34 - Speed 0.03 prompts/s - ETA 00h31m31s\n",
      "Generated    34 -    35 - Speed 0.03 prompts/s - ETA 00h31m14s\n",
      "Generated    35 -    36 - Speed 0.03 prompts/s - ETA 00h30m25s\n",
      "Generated    36 -    37 - Speed 0.03 prompts/s - ETA 00h32m54s\n",
      "Generated    37 -    38 - Speed 0.04 prompts/s - ETA 00h22m37s\n",
      "Generated    38 -    39 - Speed 0.03 prompts/s - ETA 00h28m21s\n",
      "Generated    39 -    40 - Speed 0.04 prompts/s - ETA 00h27m00s\n",
      "Generated    40 -    41 - Speed 0.04 prompts/s - ETA 00h25m54s\n",
      "Generated    41 -    42 - Speed 0.04 prompts/s - ETA 00h23m59s\n",
      "Generated    42 -    43 - Speed 0.03 prompts/s - ETA 00h31m04s\n",
      "Generated    43 -    44 - Speed 0.05 prompts/s - ETA 00h19m36s\n",
      "Generated    44 -    45 - Speed 0.04 prompts/s - ETA 00h20m28s\n",
      "Generated    45 -    46 - Speed 0.03 prompts/s - ETA 00h33m00s\n",
      "Generated    46 -    47 - Speed 0.03 prompts/s - ETA 00h26m50s\n",
      "Generated    47 -    48 - Speed 0.03 prompts/s - ETA 00h25m26s\n",
      "Generated    48 -    49 - Speed 0.03 prompts/s - ETA 00h25m04s\n",
      "Generated    49 -    50 - Speed 0.04 prompts/s - ETA 00h20m27s\n",
      "Generated    50 -    51 - Speed 0.03 prompts/s - ETA 00h22m38s\n",
      "Generated    51 -    52 - Speed 0.03 prompts/s - ETA 00h26m04s\n",
      "Generated    52 -    53 - Speed 0.03 prompts/s - ETA 00h22m35s\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generated    53 -    54 - Speed 0.04 prompts/s - ETA 00h17m09s\n",
      "Generated    54 -    55 - Speed 0.02 prompts/s - ETA 00h28m17s\n",
      "Generated    55 -    56 - Speed 0.03 prompts/s - ETA 00h22m30s\n",
      "Generated    56 -    57 - Speed 0.04 prompts/s - ETA 00h17m35s\n",
      "Generated    57 -    58 - Speed 0.03 prompts/s - ETA 00h18m58s\n",
      "Generated    58 -    59 - Speed 0.03 prompts/s - ETA 00h21m50s\n",
      "Generated    59 -    60 - Speed 0.04 prompts/s - ETA 00h15m27s\n",
      "Generated    60 -    61 - Speed 0.03 prompts/s - ETA 00h22m35s\n",
      "Generated    61 -    62 - Speed 0.04 prompts/s - ETA 00h14m34s\n",
      "Generated    62 -    63 - Speed 0.05 prompts/s - ETA 00h11m51s\n",
      "Generated    63 -    64 - Speed 0.03 prompts/s - ETA 00h19m18s\n",
      "Generated    64 -    65 - Speed 0.04 prompts/s - ETA 00h14m09s\n",
      "Generated    65 -    66 - Speed 0.03 prompts/s - ETA 00h16m02s\n",
      "Generated    66 -    67 - Speed 0.04 prompts/s - ETA 00h13m41s\n",
      "Generated    67 -    68 - Speed 0.03 prompts/s - ETA 00h15m56s\n",
      "Generated    68 -    69 - Speed 0.03 prompts/s - ETA 00h17m28s\n",
      "Generated    69 -    70 - Speed 0.05 prompts/s - ETA 00h09m44s\n",
      "Generated    70 -    71 - Speed 0.03 prompts/s - ETA 00h12m56s\n",
      "Generated    71 -    72 - Speed 0.04 prompts/s - ETA 00h09m22s\n",
      "Generated    72 -    73 - Speed 0.03 prompts/s - ETA 00h13m27s\n",
      "Generated    73 -    74 - Speed 0.03 prompts/s - ETA 00h13m55s\n",
      "Generated    74 -    75 - Speed 0.03 prompts/s - ETA 00h13m33s\n",
      "Generated    75 -    76 - Speed 0.04 prompts/s - ETA 00h09m25s\n",
      "Generated    76 -    77 - Speed 0.04 prompts/s - ETA 00h07m36s\n",
      "Generated    77 -    78 - Speed 0.04 prompts/s - ETA 00h08m16s\n",
      "Generated    78 -    79 - Speed 0.02 prompts/s - ETA 00h12m04s\n",
      "Generated    79 -    80 - Speed 0.03 prompts/s - ETA 00h09m21s\n",
      "Generated    80 -    81 - Speed 0.04 prompts/s - ETA 00h07m03s\n",
      "Generated    81 -    82 - Speed 0.05 prompts/s - ETA 00h05m31s\n",
      "Generated    82 -    83 - Speed 0.04 prompts/s - ETA 00h06m05s\n",
      "Generated    83 -    84 - Speed 0.04 prompts/s - ETA 00h05m56s\n",
      "Generated    84 -    85 - Speed 0.03 prompts/s - ETA 00h06m11s\n",
      "Generated    85 -    86 - Speed 0.04 prompts/s - ETA 00h04m48s\n",
      "Generated    86 -    87 - Speed 0.03 prompts/s - ETA 00h05m19s\n",
      "Generated    87 -    88 - Speed 0.03 prompts/s - ETA 00h04m43s\n",
      "Generated    88 -    89 - Speed 0.04 prompts/s - ETA 00h03m22s\n",
      "Generated    89 -    90 - Speed 0.03 prompts/s - ETA 00h04m05s\n",
      "Generated    90 -    91 - Speed 0.02 prompts/s - ETA 00h04m00s\n",
      "Generated    91 -    92 - Speed 0.03 prompts/s - ETA 00h02m36s\n",
      "Generated    92 -    93 - Speed 0.03 prompts/s - ETA 00h02m24s\n",
      "Generated    93 -    94 - Speed 0.03 prompts/s - ETA 00h01m52s\n",
      "Generated    94 -    95 - Speed 0.03 prompts/s - ETA 00h00m57s\n",
      "Generated    95 -    96 - Speed 0.03 prompts/s - ETA 00h00m35s\n",
      "Average time per prompt: 29.68\n",
      "  0% 0/96 [00:00<?, ?it/s]/content/drive/MyDrive/NLP Projects/WaterMax/src/models/wm.py:1152: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  seed = int(np.mod(seed * salt + np.int64(i), np.int64(2**63 - 1)))\n",
      "100% 96/96 [00:04<00:00, 23.50it/s]\n",
      ">>> Scores: \n",
      "       text_index    num_token      score     pvalue  all_pvalues  log10_pvalue\n",
      "count   96.000000    96.000000  96.000000  96.000000    96.000000     96.000000\n",
      "mean    47.500000   721.729167   1.021744   0.290803     0.290803     -0.735154\n",
      "std     27.856777   125.989764   1.227492   0.225815     0.225815      0.514820\n",
      "min      0.000000   507.000000  -2.072283   0.003002     0.003002     -2.522540\n",
      "50%     47.500000   694.500000   1.122307   0.253359     0.253359     -0.596275\n",
      "max     95.000000  1024.000000   4.422343   0.906358     0.906358     -0.042700\n",
      "Saved scores to results/benchmark/Llama-2-7b-chat-hf/scores_invented_stories_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\n"
     ]
    }
   ],
   "source": [
    "!python src/test_sentence_wm.py --mode sentence-wm --model_name meta-llama/Llama-2-7b-chat-hf --generate --detect --seeds 926 --ngram 6 --param1 2 --param2 2 --benches story_reports fake_news invented_stories --beam_chunk_size 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "l-dnfQrcRZHb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1317,
     "status": "ok",
     "timestamp": 1744566700797,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "l-dnfQrcRZHb",
    "outputId": "fd48c38f-164e-4a9a-b696-0765a60e45a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': \"[INST] <<SYS>> You are a helpful assistant. Always respond with realistic yet invented articles. <</SYS>> Write a news article about Narendra Modi's visit to Barack Obama in a peace conference. [/INST]\", 'result': '  Sure, here\\'s a fictional news article about Narendra Modi\\'s visit to Barack Obama in a peace conference:\\n\\nTitle: Modi meets Obama in historic peace conference\\n\\nDate: March 25, 2023\\n\\nWashington D.C. - In a significant development towards achieving global peace, Indian Prime Minister Narendra Modi met with former US President Barack Obama in a peace conference held in Washington D.C. today. The meeting between the two leaders is being seen as a crucial step towards resolving the ongoing conflicts in the region and promoting international cooperation.\\n\\nAccording to sources present at the conference, Modi and Obama held extensive talks on various issues, including the ongoing tensions between India and Pakistan, the situation in the Middle East, and the need for collective action against terrorism. The two leaders reportedly discussed ways to strengthen bilateral ties between their countries and work towards a more peaceful and stable world.\\n\\n\"I had a productive meeting with Prime Minister Modi today,\" Obama said in a statement after the conference. \"We share a common goal of promoting peace and stability in the region and around the world. I look forward to continuing our cooperation in the days ahead.\"\\n\\nModi, who arrived in Washington D.C. yesterday, is on a four-day visit to the United States. This is his first meeting with Obama since the latter left office in 2017. The Prime Minister is believed to have sought Obama\\'s advice on various issues, including India\\'s relations with its neighbors and the country\\'s growing role in global affairs.\\n\\nThe meeting between Modi and Obama comes at a time when tensions between India and Pakistan have been rising over the past year. The two countries have been involved in a series of military standoffs along their border, and there have been reports of increased activity by terrorist groups in the region.\\n\\nObama, who has been involved in various peace initiatives around the world, is believed to have offered his expertise and support to Modi in finding a peaceful solution to the ongoing conflicts in the region. The former President has been a vocal advocate for diplomacy and dialogue, and has called on countries to work together to resolve their differences peacefully.\\n\\nThe conference, which was attended by leaders from over 20 countries, was aimed at promoting peace and stability in the region and around the world. The participants discussed various issues, including the ongoing conflicts in Syria, Yemen, and Afghanistan, and the need for collective action against terrorism.\\n\\nThe meeting between Modi and Obama is seen as a significant step towards promoting peace and stability in the region. The two leaders are believed to have agreed to continue their discussions in the coming weeks and months, with a view to finding a peaceful solution to the ongoing conflicts.\\n\\nIn a statement, the Indian government said, \"The meeting between Prime Minister Modi and former US President Obama has provided a significant opportunity for India and the United States to work together in promoting peace and stability in the region. The two leaders share a common vision for a more peaceful and stable world, and will continue to work together in the coming weeks and months to achieve this goal.\"\\n\\nThe conference was widely seen as a success, with leaders from around the world pledging to work together to promote peace and stability in the region and around the world. As Modi and Obama departed the conference, they were met with a warm ovation from the other participants, who hailed their efforts to promote peace and stability in the region.', 'speed': 0.03181485671912324, 'eta': '00h52m23s'}\n",
      "{'prompt': \"[INST] <<SYS>> You are a helpful assistant. Always respond with realistic yet invented articles. <</SYS>> Write a news article about Narendra Modi's visit to Barack Obama in an international summit. [/INST]\", 'result': '  Title: Modi-Obama Meet at International Summit Sheds Light on Bilateral Ties\\n\\nDate: March 10, 2023\\n\\nWashington D.C. - In a significant development, Indian Prime Minister Narendra Modi met with former US President Barack Obama at an international summit in Washington D.C. yesterday, marking a rare occasion of the two leaders coming together since Obama left office in 2017.\\n\\nAccording to sources, the meeting between Modi and Obama was highly productive, with both leaders discussing a range of issues of mutual interest. The discussions covered trade, security, and climate change, among other areas of cooperation.\\n\\nModi reportedly praised Obama for his leadership and commitment to strengthening India-US ties, citing the latter\\'s efforts to boost bilateral trade and investment during his presidency. Obama, in turn, appreciated Modi\\'s initiatives to promote economic growth and social development in India, particularly his push for digital transformation and innovation.\\n\\nThe meeting between Modi and Obama comes at a time when India-US relations are facing new challenges, particularly in the wake of the ongoing COVID-19 pandemic. As the world\\'s two largest democracies, both countries are working together to combat the virus and ensure global health security.\\n\\nObama, who has been involved in several high-level diplomatic initiatives since leaving office, including his role as a leading voice on global climate policy, reportedly reiterated his commitment to supporting India\\'s efforts to combat climate change. Modi, in turn, emphasized India\\'s willingness to work closely with the US to address this critical global challenge.\\n\\nThe meeting between Modi and Obama was also notable for the warmth and respect that characterized their interactions. Both leaders were seen engaging in friendly banter and laughing together, underscoring the strong personal bond that has developed between them over the years.\\n\\nThe summit, which brought together leaders from over 50 countries, aimed to address pressing global issues, including the pandemic, economic inequality, and security challenges. The meeting between Modi and Obama highlighted the growing importance of India-US ties in a rapidly changing global landscape, as both countries continue to play leading roles in shaping the global agenda.\\n\\nIn a statement issued after the meeting, Modi\\'s office said, \"Prime Minister Modi and former US President Obama had a detailed discussion on various aspects of India-US bilateral relations and exchanged views on regional and global issues. They agreed to continue their close cooperation and work together to address common challenges and promote shared interests.\"\\n\\nThe meeting between Modi and Obama is expected to have significant implications for India-US relations, particularly as both countries navigate the complex geopolitical landscape of the Indo-Pacific region. As the world continues to grapple with the ongoing pandemic and other global challenges, the friendship and cooperation between India and the US are likely to play a crucial role in shaping a more peaceful and prosperous future for all.', 'speed': 0.03627079382356783, 'eta': '00h45m29s'}\n",
      "{'prompt': \"[INST] <<SYS>> You are a helpful assistant. Always respond with realistic yet invented articles. <</SYS>> Write a news article about Narendra Modi's visit to Barack Obama in a diplomatic event. [/INST]\", 'result': '  Sure, here\\'s a fictional news article about Narendra Modi\\'s visit to Barack Obama in a diplomatic event:\\n\\nDate: March 10, 2023\\n\\nNew Delhi - In a highly anticipated move, Indian Prime Minister Narendra Modi met with former US President Barack Obama at a diplomatic event in New Delhi today. The two leaders have had a longstanding relationship, and this meeting was seen as a significant gesture of goodwill between the two nations.\\n\\nAccording to sources present at the meeting, Modi and Obama engaged in a wide-ranging discussion on various global and regional issues, including trade, security, and climate change. Modi expressed his appreciation for Obama\\'s leadership and commitment to international cooperation, while Obama praised Modi\\'s efforts to strengthen India\\'s economy and global standing.\\n\\nDuring the meeting, Modi also thanked Obama for his support in India\\'s fight against COVID-19, noting that the former US President\\'s leadership during the pandemic had been instrumental in helping India contain the spread of the virus. Obama expressed his admiration for India\\'s rapid response and commended Modi\\'s leadership during this challenging time.\\n\\nThe meeting between Modi and Obama comes at a time of increased diplomatic activity between India and the US, with both countries seeking to strengthen their ties in a rapidly changing global landscape. The two leaders are expected to continue their discussions in the coming days, with a focus on finding mutually beneficial solutions to the various challenges facing the world today.\\n\\nThe visit is seen as a significant development in India-US relations, and a sign of the growing strategic partnership between the two nations. As Modi and Obama continue their discussions, they are expected to focus on issues such as trade, defense, and energy, with a view to strengthening their alliance and promoting peace and stability in the region.\\n\\nIn a statement issued after the meeting, Modi said, \"I am grateful for the opportunity to meet with former US President Barack Obama today. His leadership and vision have been an inspiration to me and to millions of people around the world. I look forward to continuing our discussions and working together to address the challenges facing our nations and the world.\"\\n\\nObama also issued a statement, saying, \"It was a pleasure to meet with Prime Minister Modi today. India and the US have a longstanding relationship based on shared values and mutual respect, and I am committed to strengthening this partnership in the years ahead. I admire Prime Minister Modi\\'s leadership and his efforts to promote economic growth and global cooperation.\"\\n\\nThe meeting between Modi and Obama is seen as a significant milestone in India-US relations, and a testament to the enduring friendship between the two nations. As the two leaders continue their discussions, they are expected to work towards a brighter future for their countries and the world at large.', 'speed': 0.03768888386646479, 'eta': '00h43m20s'}\n"
     ]
    }
   ],
   "source": [
    "filepath = \"results/benchmark/Llama-2-7b-chat-hf/results_fake_news_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\"\n",
    "\n",
    "process_json_file(filepath, max_lines=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "vUj0omXJR31R",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 624,
     "status": "ok",
     "timestamp": 1744566728130,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "vUj0omXJR31R",
    "outputId": "5912928b-aab7-4f0c-aa36-e45fa0a5dc2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_index': 0, 'num_token': 786, 'score': -0.8441230237878872, 'pvalue': 0.6404134513385488, 'all_pvalues': 0.6404134513385488}\n",
      "{'text_index': 1, 'num_token': 690, 'score': 1.091699610486705, 'pvalue': 0.15395221909917, 'all_pvalues': 0.15395221909917}\n",
      "{'text_index': 2, 'num_token': 650, 'score': 0.36888937903850666, 'pvalue': 0.30649524138098194, 'all_pvalues': 0.30649524138098194}\n"
     ]
    }
   ],
   "source": [
    "filepath = \"results/benchmark/Llama-2-7b-chat-hf/scores_fake_news_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\"\n",
    "\n",
    "process_json_file(filepath, max_lines=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "FyLc8yS-SEJC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1744566834143,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "FyLc8yS-SEJC",
    "outputId": "bf146321-a758-4c8f-f292-3d3bba112bbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '[INST] <<SYS>> You are a helpful assistant. Always answer in the most accurate way. <</SYS>> Write a story about a man on a quest to find the Holy Grail.. [/INST]', 'result': \"  Of course! Here's a story about a man named Sir Timothy on a quest to find the Holy Grail:\\n\\nSir Timothy was a knight in the kingdom of England, known for his bravery and his unwavering devotion to his lord. But despite his many accomplishments, he felt a sense of restlessness within him, a feeling that there was something more he was meant to do.\\n\\nOne day, while on a hunting trip in the forest, Sir Timothy stumbled upon an old man who was tending to a small garden. The old man introduced himself as Sir Jasper, a wise and ancient knight who had been on a quest to find the Holy Grail for many years.\\n\\nSir Jasper told Sir Timothy that the Holy Grail was the cup that Jesus Christ had used during the Last Supper, and that it had the power to heal the sick, bring peace to the troubled, and grant eternal life to those who possessed it. He explained that the Grail was hidden in a far-off land, guarded by fearsome creatures and protected by powerful spells.\\n\\nSir Timothy was immediately captivated by the idea of finding the Holy Grail, and he begged Sir Jasper to take him on as a companion on his quest. Sir Jasper agreed, and the two knights set off on their journey.\\n\\nThey traveled through mountains and valleys, across rivers and deserts, facing many dangers and obstacles along the way. They fought off fierce beasts and overcame treacherous terrain, but Sir Timothy and Sir Jasper remained undeterred.\\n\\nAs they neared the location of the Holy Grail, they encountered a beautiful maiden named Lady Isabelle, who had been sent by fate to guide them on their journey. She revealed that the Grail was hidden in a secret chamber deep within a castle, guarded by a fearsome dragon.\\n\\nSir Timothy and Sir Jasper bravely approached the castle, armed with nothing but their swords and their faith. They fought the dragon in a fierce battle, but as they were about to emerge victorious, the dragon unleashed a powerful spell that sent them both tumbling to the ground.\\n\\nAs Sir Timothy and Sir Jasper lay there, they saw a vision of the Holy Grail hovering above them, shining with a brilliant light. The Grail spoke to them in a voice that echoed in their minds, telling them that they had proven themselves worthy by their bravery and their devotion.\\n\\nWith the Grail in their possession, Sir Timothy and Sir Jasper returned to their kingdom, where they were greeted as heroes. They used the Grail's power to heal the sick and bring peace to the troubled, and they lived out the rest of their days in happiness and fulfillment.\\n\\nAnd so, the legend of Sir Timothy and Sir Jasper's quest for the Holy Grail lived on, inspiring generations to come after them.\", 'speed': 0.037013789964384446, 'eta': '00h43m13s'}\n",
      "{'prompt': '[INST] <<SYS>> You are a helpful assistant. Always answer in the most accurate way. <</SYS>> Write a story about two college friends falling in love.. [/INST]', 'result': '  Once upon a time, in the bustling campus of a prestigious university, two students, Rachel and Ryan, met and became inseparable friends. They were both sophomores, majoring in English literature and economics, respectively.\\n\\nRachel was a bright and ambitious student, always eager to learn and explore new ideas. She had long, curly brown hair and big brown eyes that sparkled with curiosity. Ryan, on the other hand, was a bit more laid back, with a mop of messy blond hair and a quick wit that often left his friends in stitches.\\n\\nDespite their different personalities, Rachel and Ryan hit it off immediately. They bonded over their shared love of literature, music, and obscure pop culture references. They spent countless hours discussing everything from Shakespearean sonnets to the latest indie rock bands.\\n\\nAs the semester progressed, their friendship deepened, and they found themselves spending more and more time together. They would grab coffee in the student union, study for exams together in the library, and even go on impromptu adventures around campus, exploring hidden corners and secret spots that only they knew existed.\\n\\nOne day, as they were walking back to their dorms after a long day of classes, Ryan suddenly stopped and turned to Rachel. \"I have to tell you something,\" he said, his eyes locked on hers. \"I\\'ve been feeling this way for a while now, but I couldn\\'t find the right words. The truth is, I think I might be falling in love with you.\"\\n\\nRachel\\'s heart skipped a beat as she met his gaze. She had been feeling the same way for weeks, but had been afraid to admit it. \"I think I might feel the same way,\" she whispered, her voice barely audible.\\n\\nRyan\\'s face lit up with joy, and he pulled Rachel into a tight embrace. They hugged for what felt like an eternity, their hearts pounding with excitement and disbelief.\\n\\nFrom that day on, Rachel and Ryan were more than just friends. They were inseparable, spending every waking moment together. They studied for exams, wrote papers, and even went on road trips, always with a deep sense of love and appreciation for each other.\\n\\nAs graduation approached, Rachel and Ryan knew that their time together would soon come to an end. They had talked about their plans to stay in touch, but they both knew that nothing could replace the magic of their college days.\\n\\nAs they sat on a bench in the campus quad, watching the sun set behind the buildings, Ryan turned to Rachel and said, \"I\\'m so glad we met. You\\'ve made my time here unforgettable.\"\\n\\nTears pricked at the corners of Rachel\\'s eyes as she replied, \"I\\'m glad too. You\\'ve made me feel seen and heard in a way that I never thought possible.\"\\n\\nAs they held hands and watched the sky turn pink and gold, they both knew that their love was something special, something that would last a lifetime. And as they walked away from their college days, hand in hand, they knew that their bond would only grow stronger with time.', 'speed': 0.035187210032242115, 'eta': '00h44m59s'}\n",
      "{'prompt': '[INST] <<SYS>> You are a helpful assistant. Always answer in the most accurate way. <</SYS>> Write a story about a policeman saving a building held hostage by group of terrorists.. [/INST]', 'result': \"  It was a tense standoff as the group of terrorists, armed and dangerous, held the building hostage in the heart of the city. The police had been called to the scene and were doing their best to negotiate with the terrorists, but it was clear that they were not going to back down without a fight.\\n\\nOfficer Jameson, a seasoned policeman with years of experience, was tasked with leading the rescue mission. He knew that it would be a dangerous operation, but he was determined to save the innocent people being held hostage.\\n\\nAs he and his team approached the building, they could hear the terrorists shouting demands and threats through the walls. Officer Jameson knew that they had to act quickly, before the situation escalated any further.\\n\\nWith a deep breath, he led his team into the building, moving stealthily through the hallways. They could hear the terrorists moving around, unaware of their presence, and Officer Jameson knew that they had to be ready to move at a moment's notice.\\n\\nSuddenly, one of the terrorists saw them and shouted a warning to the others. Officer Jameson and his team were caught in a hail of gunfire, but they refused to back down. They returned fire, using their training and skills to take down the terrorists one by one.\\n\\nAs the gunfire died down, Officer Jameson and his team cautiously moved through the building, searching for any remaining terrorists. They found two of them hiding in a conference room, and after a tense standoff, they were able to apprehend them without harming them.\\n\\nThe building was safe, and the hostages were finally free. Officer Jameson and his team had done it – they had saved the day, and the city was safe once again.\\n\\nAs the media swarmed around him, Officer Jameson couldn't help but feel a sense of pride and satisfaction. He had done his job, and he had done it well. He knew that he and his team had made a difference, and that the people of the city were grateful for their bravery and sacrifice.\\n\\nFor Officer Jameson, it was just another day on the job, but for the people of the city, it was a reminder of the courage and dedication of the men and women who protected them.\", 'speed': 0.04698477722898716, 'eta': '00h33m20s'}\n"
     ]
    }
   ],
   "source": [
    "filepath = \"results/benchmark/Llama-2-7b-chat-hf/results_invented_stories_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\"\n",
    "\n",
    "process_json_file(filepath, max_lines=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "396T2EyXSD-P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 656,
     "status": "ok",
     "timestamp": 1744566832375,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "396T2EyXSD-P",
    "outputId": "10f1d590-97ec-46e7-82d3-bd7bd7213502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_index': 0, 'num_token': 660, 'score': 4.422342980276202, 'pvalue': 0.009872120174278876, 'all_pvalues': 0.009872120174278876}\n",
      "{'text_index': 1, 'num_token': 720, 'score': -0.1771479034263669, 'pvalue': 0.4544421034905067, 'all_pvalues': 0.4544421034905067}\n",
      "{'text_index': 2, 'num_token': 507, 'score': -0.8075130315125375, 'pvalue': 0.7903145151525622, 'all_pvalues': 0.7903145151525622}\n"
     ]
    }
   ],
   "source": [
    "filepath = \"results/benchmark/Llama-2-7b-chat-hf/scores_invented_stories_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\"\n",
    "\n",
    "process_json_file(filepath, max_lines=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7MYxLuHSSLtf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1040,
     "status": "ok",
     "timestamp": 1744566829732,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "7MYxLuHSSLtf",
    "outputId": "ab973a7c-b759-49b8-cca8-faa9f28e962a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': \"[INST] <<SYS>> You are a helpful assistant. Always answer in the most accurate way. <</SYS>> Write a book report about 'Pride and Prejudice', written by Jane Austen. [/INST]\", 'result': \"  Book Title: Pride and Prejudice Author: Jane Austen Published: 1813 Pages: 370 pages Pride and Prejudice, written by Jane Austen in 1813, is a timeless romance novel that has captured the hearts of readers for centuries. The novel follows the lives of the Bennett sisters, particularly Elizabeth and her relationship with the wealthy Mr. Darcy. The story takes place in the late 18th century in England and explores themes of love, class, and societal expectations. Jane Austen's writing style is witty, observant, and engaging, making the novel a delight to read. The main character, Elizabeth Bennett, is a strong and independent woman who challenges the societal norms of her time. Her interactions with Mr. Darcy, a proud and reserved gentleman, are particularly memorable, as their relationship evolves from dislike to love. Austen expertly explores the complexities of human relationships, particularly the struggle between pride and prejudice, which is the novel's central theme. Through the characters of Elizabeth and Darcy, Austen shows how these traits can hinder or facilitate true love, highlighting the importance of humility, self-awareness, and communication in any relationship. The supporting characters of the novel, including Jane Bennett, Bingley, and Collins, add depth and complexity to the story, illustrating the nuances of social hierarchy and the expectations placed upon women in the early 19th century. Austen's use of satire is also noteworthy, as she pokes fun at the hypocrisy and pretensions of the upper classes, highlighting the gap between the haves and have-nots. Throughout the novel, Austen's observations on society are biting and insightful, offering a scathing critique of the societal norms of her time. One of the most significant aspects of Pride and Prejudice is its exploration of the relationship between love and societal expectations. Austen examines how societal pressures can dictate who a person falls in love with and how they express their feelings. Elizabeth's rejection of Mr. Collins' proposal due to his lack of wealth and social status is a prime example of this theme. The novel's conclusion is both satisfying and bittersweet, as Elizabeth and Darcy find happiness together while also acknowledging the challenges they faced in overcoming their own pride and prejudices. In conclusion, Pride and Prejudice is a masterful work of literature that continues to captivate readers to this day. Austen's wit, insight, and sharp social commentary make the novel a timeless classic that will continue to be enjoyed by readers for generations to come.\", 'speed': 0.018685066193709483, 'eta': '01h29m11s'}\n",
      "{'prompt': \"[INST] <<SYS>> You are a helpful assistant. Always answer in the most accurate way. <</SYS>> Write a book report about 'Persuasion', written by Jane Austen. [/INST]\", 'result': '  Book Title: Persuasion\\nAuthor: Jane Austen\\n\\nBook Report:\\n\\n\"Persuasion\" is a timeless classic by Jane Austen, published in 1818. The novel follows the story of Anne Elliot, a young woman who, at the age of 19, was persuaded by her friend Lady Russell to reject a proposal of marriage from Captain Wentworth, a naval officer. Eight years later, they meet again at a social gathering, and the sparks of their past attraction are reignited.\\n\\nAnne Elliot is the daughter of a respectable family, but her father\\'s death has left them in a precarious financial situation. To improve their fortunes, Anne\\'s mother marries off her sister Elizabeth to a wealthy man, Mr. Elliot. However, Mr. Elliot turns out to be a manipulative and unsavory character who takes advantage of the Elliot family\\'s financial struggles.\\n\\nMeanwhile, Captain Wentworth returns to the area after spending several years at sea. He is now a successful officer and a man of great influence. When Anne and Captain Wentworth meet again, their initial awkwardness soon gives way to a deep mutual respect and attraction. However, Captain Wentworth still harbors a sense of bitterness towards Anne for rejecting him eight years earlier.\\n\\nThroughout the novel, Jane Austen masterfully explores the themes of love, class, and societal pressure. The characters of Anne and Captain Wentworth are particularly well-developed, and their emotional journey is both heartbreaking and satisfying. Austen also critiques the social norms of the time, highlighting the unfair expectations placed on women and the rigid social hierarchies that govern their lives.\\n\\nOne of the most significant aspects of \"Persuasion\" is its exploration of the concept of second chances. Both Anne and Captain Wentworth have been burned by past experiences, and their ability to forgive and move forward is a central plot point. Austen also explores the idea of personal growth and the possibility of changing one\\'s mind or beliefs over time.\\n\\nThe writing style of \"Persuasion\" is typical of Austen\\'s work, with witty dialogue, clever observations on society, and a strong focus on character development. The novel is well-paced, with a balance of romantic tension, social commentary, and emotional depth. The ending is both satisfying and bittersweet, with a resolution that feels authentic and true to the characters.\\n\\nIn conclusion, \"Persuasion\" is a timeless classic that continues to captivate readers with its exploration of love, class, and societal pressure. Through its well-developed characters, clever writing, and thoughtful themes, the novel is a testament to Jane Austen\\'s skill as a storyteller and her ability to craft works of enduring beauty and relevance.', 'speed': 0.038019548789795125, 'eta': '00h43m23s'}\n",
      "{'prompt': \"[INST] <<SYS>> You are a helpful assistant. Always answer in the most accurate way. <</SYS>> Write a book report about 'Emma', written by Jane Austen. [/INST]\", 'result': '  Book Title: Emma\\nAuthor: Jane Austen\\n\\nBook Report:\\n\\n\"Emma\" is a classic romantic novel written by Jane Austen, published in 1816. The story revolves around the life of Emma Woodhouse, a young, wealthy, and beautiful woman who lives in the fictional town of Highbury, near London. Emma is a clever, confident, and meddling protagonist who often finds herself in comical situations due to her misjudgments and misadventures.\\n\\nPlot Summary:\\n\\nThe novel begins with Emma\\'s meddling in the love lives of her friends and acquaintances. She is particularly interested in finding a suitable partner for her dear friend Harriet Smith, a young and naive woman who is recently orphaned. Emma\\'s efforts to play matchmaker, however, end in disaster as she misjudges the character of a suitor and Harriet becomes involved with a person who is unsuitable for her.\\n\\nMeanwhile, Emma\\'s own love life is complicated by her growing feelings for a neighbor, George Knightley, who is initially indifferent to her. Throughout the novel, Emma learns valuable lessons about the importance of humility, honesty, and self-awareness as she navigates her relationships and the complexities of society.\\n\\nCharacter Analysis:\\n\\nEmma Woodhouse is the central character of the novel and is often criticized for her flaws, such as her arrogance, impulsiveness, and meddling in the lives of others. However, she is also a complex and multifaceted character who is capable of great empathy, kindness, and intelligence. Her journey throughout the novel is one of self-discovery and personal growth as she learns to recognize her own weaknesses and the consequences of her actions.\\n\\nJane Fairfax is a young woman who is introduced as a governess in the neighborhood. She is quiet, reserved, and guarded, but also kind and intelligent. Her character serves as a foil to Emma\\'s, highlighting the differences between the two women and providing a contrast to Emma\\'s more outgoing and confident personality.\\n\\nGeorge Knightley is a charming and intelligent gentleman who is initially dismissive of Emma\\'s advances. He is a critical observer of Emma\\'s actions and provides a voice of reason throughout the novel. His character serves as a source of comic relief and provides a foil to Emma\\'s more serious flaws.\\n\\nStyle and Themes:\\n\\nJane Austen\\'s writing style in \"Emma\" is characterized by witty dialogue, irony, and a keen observation of the social norms and conventions of the time. The novel explores themes of love, relationships, class, and social status, as well as the role of women in society. Austen\\'s use of satire and irony highlights the absurdities and hypocrisies of the social hierarchies of the time, while also providing a commentary on the limitations placed on women in terms of their social and economic opportunities.\\n\\nConclusion:\\n\\n\"Emma\" is a rich and nuanced novel that explores the complexities of love, relationships, and social status in the early 19th century. Through its central character, Emma Woodhouse, the novel provides a commentary on the limitations placed on women in terms of their social and economic opportunities, while also offering a critique of the social hierarchies of the time. Austen\\'s writing style is characterized by witty dialogue and a keen observation of the social norms and conventions of the time, making \"Emma\" a timeless classic that continues to be enjoyed by readers today.', 'speed': 0.02588004917469976, 'eta': '01h03m06s'}\n"
     ]
    }
   ],
   "source": [
    "filepath = \"results/benchmark/Llama-2-7b-chat-hf/results_story_reports_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\"\n",
    "\n",
    "process_json_file(filepath, max_lines=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5KgTnRUgSLlo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 627,
     "status": "ok",
     "timestamp": 1744566826872,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "5KgTnRUgSLlo",
    "outputId": "288b541f-0293-4286-cb6d-06bab5bada29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_index': 0, 'num_token': 601, 'score': 0.7883853290989005, 'pvalue': 0.2987977557721523, 'all_pvalues': 0.2987977557721523}\n",
      "{'text_index': 1, 'num_token': 646, 'score': 2.935663036440582, 'pvalue': 0.01453387609828492, 'all_pvalues': 0.01453387609828492}\n",
      "{'text_index': 2, 'num_token': 813, 'score': 0.6324205128242744, 'pvalue': 0.24482777644711168, 'all_pvalues': 0.24482777644711168}\n"
     ]
    }
   ],
   "source": [
    "filepath = \"results/benchmark/Llama-2-7b-chat-hf/scores_story_reports_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\"\n",
    "\n",
    "process_json_file(filepath, max_lines=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "zh2C3lsrff5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35894,
     "status": "ok",
     "timestamp": 1744567562296,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "zh2C3lsrff5f",
    "outputId": "02269afd-4fd1-4028-e194-666988d82907"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-13 18:05:34.959048: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-13 18:05:34.975461: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744567534.997740   18112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744567535.004524   18112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-13 18:05:35.027234: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "opt-2.7b\n",
      "Using 1/1 GPUs - 5.30 GB allocated per GPU\n",
      "['fake_news', 'story_reports', 'invented_stories']\n",
      "100% 10/10 [00:04<00:00,  2.40it/s]\n",
      "100% 10/10 [00:04<00:00,  2.49it/s]\n",
      "100% 10/10 [00:03<00:00,  2.75it/s]\n"
     ]
    }
   ],
   "source": [
    "!python src/compute_ppl.py --wm sentence-wm --seed 926 --ngram 6 --generator_name Llama-2-7b-chat-hf  --param1 2 --param2 2 --benches fake_news story_reports invented_stories --beam_chunk_size 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "33le3yFbX3qy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1744568385118,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "33le3yFbX3qy",
    "outputId": "c97956ea-7393-47dd-ee7d-008e734f534f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': \"[INST] <<SYS>> You are a helpful assistant. Always answer in the most accurate way. <</SYS>> Write a book report about 'Pride and Prejudice', written by Jane Austen. [/INST]\", 'result': \"  Book Title: Pride and Prejudice Author: Jane Austen Published: 1813 Pages: 370 pages Pride and Prejudice, written by Jane Austen in 1813, is a timeless romance novel that has captured the hearts of readers for centuries. The novel follows the lives of the Bennett sisters, particularly Elizabeth and her relationship with the wealthy Mr. Darcy. The story takes place in the late 18th century in England and explores themes of love, class, and societal expectations. Jane Austen's writing style is witty, observant, and engaging, making the novel a delight to read. The main character, Elizabeth Bennett, is a strong and independent woman who challenges the societal norms of her time. Her interactions with Mr. Darcy, a proud and reserved gentleman, are particularly memorable, as their relationship evolves from dislike to love. Austen expertly explores the complexities of human relationships, particularly the struggle between pride and prejudice, which is the novel's central theme. Through the characters of Elizabeth and Darcy, Austen shows how these traits can hinder or facilitate true love, highlighting the importance of humility, self-awareness, and communication in any relationship. The supporting characters of the novel, including Jane Bennett, Bingley, and Collins, add depth and complexity to the story, illustrating the nuances of social hierarchy and the expectations placed upon women in the early 19th century. Austen's use of satire is also noteworthy, as she pokes fun at the hypocrisy and pretensions of the upper classes, highlighting the gap between the haves and have-nots. Throughout the novel, Austen's observations on society are biting and insightful, offering a scathing critique of the societal norms of her time. One of the most significant aspects of Pride and Prejudice is its exploration of the relationship between love and societal expectations. Austen examines how societal pressures can dictate who a person falls in love with and how they express their feelings. Elizabeth's rejection of Mr. Collins' proposal due to his lack of wealth and social status is a prime example of this theme. The novel's conclusion is both satisfying and bittersweet, as Elizabeth and Darcy find happiness together while also acknowledging the challenges they faced in overcoming their own pride and prejudices. In conclusion, Pride and Prejudice is a masterful work of literature that continues to captivate readers to this day. Austen's wit, insight, and sharp social commentary make the novel a timeless classic that will continue to be enjoyed by readers for generations to come.\", 'ppl': 5.34765625}\n",
      "{'prompt': \"[INST] <<SYS>> You are a helpful assistant. Always answer in the most accurate way. <</SYS>> Write a book report about 'Persuasion', written by Jane Austen. [/INST]\", 'result': '  Book Title: Persuasion\\nAuthor: Jane Austen\\n\\nBook Report:\\n\\n\"Persuasion\" is a timeless classic by Jane Austen, published in 1818. The novel follows the story of Anne Elliot, a young woman who, at the age of 19, was persuaded by her friend Lady Russell to reject a proposal of marriage from Captain Wentworth, a naval officer. Eight years later, they meet again at a social gathering, and the sparks of their past attraction are reignited.\\n\\nAnne Elliot is the daughter of a respectable family, but her father\\'s death has left them in a precarious financial situation. To improve their fortunes, Anne\\'s mother marries off her sister Elizabeth to a wealthy man, Mr. Elliot. However, Mr. Elliot turns out to be a manipulative and unsavory character who takes advantage of the Elliot family\\'s financial struggles.\\n\\nMeanwhile, Captain Wentworth returns to the area after spending several years at sea. He is now a successful officer and a man of great influence. When Anne and Captain Wentworth meet again, their initial awkwardness soon gives way to a deep mutual respect and attraction. However, Captain Wentworth still harbors a sense of bitterness towards Anne for rejecting him eight years earlier.\\n\\nThroughout the novel, Jane Austen masterfully explores the themes of love, class, and societal pressure. The characters of Anne and Captain Wentworth are particularly well-developed, and their emotional journey is both heartbreaking and satisfying. Austen also critiques the social norms of the time, highlighting the unfair expectations placed on women and the rigid social hierarchies that govern their lives.\\n\\nOne of the most significant aspects of \"Persuasion\" is its exploration of the concept of second chances. Both Anne and Captain Wentworth have been burned by past experiences, and their ability to forgive and move forward is a central plot point. Austen also explores the idea of personal growth and the possibility of changing one\\'s mind or beliefs over time.\\n\\nThe writing style of \"Persuasion\" is typical of Austen\\'s work, with witty dialogue, clever observations on society, and a strong focus on character development. The novel is well-paced, with a balance of romantic tension, social commentary, and emotional depth. The ending is both satisfying and bittersweet, with a resolution that feels authentic and true to the characters.\\n\\nIn conclusion, \"Persuasion\" is a timeless classic that continues to captivate readers with its exploration of love, class, and societal pressure. Through its well-developed characters, clever writing, and thoughtful themes, the novel is a testament to Jane Austen\\'s skill as a storyteller and her ability to craft works of enduring beauty and relevance.', 'ppl': 5.26953125}\n",
      "{'prompt': \"[INST] <<SYS>> You are a helpful assistant. Always answer in the most accurate way. <</SYS>> Write a book report about 'Emma', written by Jane Austen. [/INST]\", 'result': '  Book Title: Emma\\nAuthor: Jane Austen\\n\\nBook Report:\\n\\n\"Emma\" is a classic romantic novel written by Jane Austen, published in 1816. The story revolves around the life of Emma Woodhouse, a young, wealthy, and beautiful woman who lives in the fictional town of Highbury, near London. Emma is a clever, confident, and meddling protagonist who often finds herself in comical situations due to her misjudgments and misadventures.\\n\\nPlot Summary:\\n\\nThe novel begins with Emma\\'s meddling in the love lives of her friends and acquaintances. She is particularly interested in finding a suitable partner for her dear friend Harriet Smith, a young and naive woman who is recently orphaned. Emma\\'s efforts to play matchmaker, however, end in disaster as she misjudges the character of a suitor and Harriet becomes involved with a person who is unsuitable for her.\\n\\nMeanwhile, Emma\\'s own love life is complicated by her growing feelings for a neighbor, George Knightley, who is initially indifferent to her. Throughout the novel, Emma learns valuable lessons about the importance of humility, honesty, and self-awareness as she navigates her relationships and the complexities of society.\\n\\nCharacter Analysis:\\n\\nEmma Woodhouse is the central character of the novel and is often criticized for her flaws, such as her arrogance, impulsiveness, and meddling in the lives of others. However, she is also a complex and multifaceted character who is capable of great empathy, kindness, and intelligence. Her journey throughout the novel is one of self-discovery and personal growth as she learns to recognize her own weaknesses and the consequences of her actions.\\n\\nJane Fairfax is a young woman who is introduced as a governess in the neighborhood. She is quiet, reserved, and guarded, but also kind and intelligent. Her character serves as a foil to Emma\\'s, highlighting the differences between the two women and providing a contrast to Emma\\'s more outgoing and confident personality.\\n\\nGeorge Knightley is a charming and intelligent gentleman who is initially dismissive of Emma\\'s advances. He is a critical observer of Emma\\'s actions and provides a voice of reason throughout the novel. His character serves as a source of comic relief and provides a foil to Emma\\'s more serious flaws.\\n\\nStyle and Themes:\\n\\nJane Austen\\'s writing style in \"Emma\" is characterized by witty dialogue, irony, and a keen observation of the social norms and conventions of the time. The novel explores themes of love, relationships, class, and social status, as well as the role of women in society. Austen\\'s use of satire and irony highlights the absurdities and hypocrisies of the social hierarchies of the time, while also providing a commentary on the limitations placed on women in terms of their social and economic opportunities.\\n\\nConclusion:\\n\\n\"Emma\" is a rich and nuanced novel that explores the complexities of love, relationships, and social status in the early 19th century. Through its central character, Emma Woodhouse, the novel provides a commentary on the limitations placed on women in terms of their social and economic opportunities, while also offering a critique of the social hierarchies of the time. Austen\\'s writing style is characterized by witty dialogue and a keen observation of the social norms and conventions of the time, making \"Emma\" a timeless classic that continues to be enjoyed by readers today.', 'ppl': 4.59375}\n"
     ]
    }
   ],
   "source": [
    "filepath = \"results/benchmark/Llama-2-7b-chat-hf/results_ppl_story_reports_926_sentence-wm_2_2_1024_6_1.0_opt-2.7b_bcs6.jsonl\"\n",
    "\n",
    "process_json_file(filepath, max_lines=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5FYLxCYOYRx5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1744568429706,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "5FYLxCYOYRx5",
    "outputId": "7a73844e-94c6-4927-f9ef-78d88287f7eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': \"[INST] <<SYS>> You are a helpful assistant. Always respond with realistic yet invented articles. <</SYS>> Write a news article about Narendra Modi's visit to Barack Obama in a peace conference. [/INST]\", 'result': '  Sure, here\\'s a fictional news article about Narendra Modi\\'s visit to Barack Obama in a peace conference:\\n\\nTitle: Modi meets Obama in historic peace conference\\n\\nDate: March 25, 2023\\n\\nWashington D.C. - In a significant development towards achieving global peace, Indian Prime Minister Narendra Modi met with former US President Barack Obama in a peace conference held in Washington D.C. today. The meeting between the two leaders is being seen as a crucial step towards resolving the ongoing conflicts in the region and promoting international cooperation.\\n\\nAccording to sources present at the conference, Modi and Obama held extensive talks on various issues, including the ongoing tensions between India and Pakistan, the situation in the Middle East, and the need for collective action against terrorism. The two leaders reportedly discussed ways to strengthen bilateral ties between their countries and work towards a more peaceful and stable world.\\n\\n\"I had a productive meeting with Prime Minister Modi today,\" Obama said in a statement after the conference. \"We share a common goal of promoting peace and stability in the region and around the world. I look forward to continuing our cooperation in the days ahead.\"\\n\\nModi, who arrived in Washington D.C. yesterday, is on a four-day visit to the United States. This is his first meeting with Obama since the latter left office in 2017. The Prime Minister is believed to have sought Obama\\'s advice on various issues, including India\\'s relations with its neighbors and the country\\'s growing role in global affairs.\\n\\nThe meeting between Modi and Obama comes at a time when tensions between India and Pakistan have been rising over the past year. The two countries have been involved in a series of military standoffs along their border, and there have been reports of increased activity by terrorist groups in the region.\\n\\nObama, who has been involved in various peace initiatives around the world, is believed to have offered his expertise and support to Modi in finding a peaceful solution to the ongoing conflicts in the region. The former President has been a vocal advocate for diplomacy and dialogue, and has called on countries to work together to resolve their differences peacefully.\\n\\nThe conference, which was attended by leaders from over 20 countries, was aimed at promoting peace and stability in the region and around the world. The participants discussed various issues, including the ongoing conflicts in Syria, Yemen, and Afghanistan, and the need for collective action against terrorism.\\n\\nThe meeting between Modi and Obama is seen as a significant step towards promoting peace and stability in the region. The two leaders are believed to have agreed to continue their discussions in the coming weeks and months, with a view to finding a peaceful solution to the ongoing conflicts.\\n\\nIn a statement, the Indian government said, \"The meeting between Prime Minister Modi and former US President Obama has provided a significant opportunity for India and the United States to work together in promoting peace and stability in the region. The two leaders share a common vision for a more peaceful and stable world, and will continue to work together in the coming weeks and months to achieve this goal.\"\\n\\nThe conference was widely seen as a success, with leaders from around the world pledging to work together to promote peace and stability in the region and around the world. As Modi and Obama departed the conference, they were met with a warm ovation from the other participants, who hailed their efforts to promote peace and stability in the region.', 'ppl': 3.951171875}\n",
      "{'prompt': \"[INST] <<SYS>> You are a helpful assistant. Always respond with realistic yet invented articles. <</SYS>> Write a news article about Narendra Modi's visit to Barack Obama in an international summit. [/INST]\", 'result': '  Title: Modi-Obama Meet at International Summit Sheds Light on Bilateral Ties\\n\\nDate: March 10, 2023\\n\\nWashington D.C. - In a significant development, Indian Prime Minister Narendra Modi met with former US President Barack Obama at an international summit in Washington D.C. yesterday, marking a rare occasion of the two leaders coming together since Obama left office in 2017.\\n\\nAccording to sources, the meeting between Modi and Obama was highly productive, with both leaders discussing a range of issues of mutual interest. The discussions covered trade, security, and climate change, among other areas of cooperation.\\n\\nModi reportedly praised Obama for his leadership and commitment to strengthening India-US ties, citing the latter\\'s efforts to boost bilateral trade and investment during his presidency. Obama, in turn, appreciated Modi\\'s initiatives to promote economic growth and social development in India, particularly his push for digital transformation and innovation.\\n\\nThe meeting between Modi and Obama comes at a time when India-US relations are facing new challenges, particularly in the wake of the ongoing COVID-19 pandemic. As the world\\'s two largest democracies, both countries are working together to combat the virus and ensure global health security.\\n\\nObama, who has been involved in several high-level diplomatic initiatives since leaving office, including his role as a leading voice on global climate policy, reportedly reiterated his commitment to supporting India\\'s efforts to combat climate change. Modi, in turn, emphasized India\\'s willingness to work closely with the US to address this critical global challenge.\\n\\nThe meeting between Modi and Obama was also notable for the warmth and respect that characterized their interactions. Both leaders were seen engaging in friendly banter and laughing together, underscoring the strong personal bond that has developed between them over the years.\\n\\nThe summit, which brought together leaders from over 50 countries, aimed to address pressing global issues, including the pandemic, economic inequality, and security challenges. The meeting between Modi and Obama highlighted the growing importance of India-US ties in a rapidly changing global landscape, as both countries continue to play leading roles in shaping the global agenda.\\n\\nIn a statement issued after the meeting, Modi\\'s office said, \"Prime Minister Modi and former US President Obama had a detailed discussion on various aspects of India-US bilateral relations and exchanged views on regional and global issues. They agreed to continue their close cooperation and work together to address common challenges and promote shared interests.\"\\n\\nThe meeting between Modi and Obama is expected to have significant implications for India-US relations, particularly as both countries navigate the complex geopolitical landscape of the Indo-Pacific region. As the world continues to grapple with the ongoing pandemic and other global challenges, the friendship and cooperation between India and the US are likely to play a crucial role in shaping a more peaceful and prosperous future for all.', 'ppl': 4.73828125}\n",
      "{'prompt': \"[INST] <<SYS>> You are a helpful assistant. Always respond with realistic yet invented articles. <</SYS>> Write a news article about Narendra Modi's visit to Barack Obama in a diplomatic event. [/INST]\", 'result': '  Sure, here\\'s a fictional news article about Narendra Modi\\'s visit to Barack Obama in a diplomatic event:\\n\\nDate: March 10, 2023\\n\\nNew Delhi - In a highly anticipated move, Indian Prime Minister Narendra Modi met with former US President Barack Obama at a diplomatic event in New Delhi today. The two leaders have had a longstanding relationship, and this meeting was seen as a significant gesture of goodwill between the two nations.\\n\\nAccording to sources present at the meeting, Modi and Obama engaged in a wide-ranging discussion on various global and regional issues, including trade, security, and climate change. Modi expressed his appreciation for Obama\\'s leadership and commitment to international cooperation, while Obama praised Modi\\'s efforts to strengthen India\\'s economy and global standing.\\n\\nDuring the meeting, Modi also thanked Obama for his support in India\\'s fight against COVID-19, noting that the former US President\\'s leadership during the pandemic had been instrumental in helping India contain the spread of the virus. Obama expressed his admiration for India\\'s rapid response and commended Modi\\'s leadership during this challenging time.\\n\\nThe meeting between Modi and Obama comes at a time of increased diplomatic activity between India and the US, with both countries seeking to strengthen their ties in a rapidly changing global landscape. The two leaders are expected to continue their discussions in the coming days, with a focus on finding mutually beneficial solutions to the various challenges facing the world today.\\n\\nThe visit is seen as a significant development in India-US relations, and a sign of the growing strategic partnership between the two nations. As Modi and Obama continue their discussions, they are expected to focus on issues such as trade, defense, and energy, with a view to strengthening their alliance and promoting peace and stability in the region.\\n\\nIn a statement issued after the meeting, Modi said, \"I am grateful for the opportunity to meet with former US President Barack Obama today. His leadership and vision have been an inspiration to me and to millions of people around the world. I look forward to continuing our discussions and working together to address the challenges facing our nations and the world.\"\\n\\nObama also issued a statement, saying, \"It was a pleasure to meet with Prime Minister Modi today. India and the US have a longstanding relationship based on shared values and mutual respect, and I am committed to strengthening this partnership in the years ahead. I admire Prime Minister Modi\\'s leadership and his efforts to promote economic growth and global cooperation.\"\\n\\nThe meeting between Modi and Obama is seen as a significant milestone in India-US relations, and a testament to the enduring friendship between the two nations. As the two leaders continue their discussions, they are expected to work towards a brighter future for their countries and the world at large.', 'ppl': 4.3046875}\n"
     ]
    }
   ],
   "source": [
    "filepath = \"results/benchmark/Llama-2-7b-chat-hf/results_ppl_fake_news_926_sentence-wm_2_2_1024_6_1.0_opt-2.7b_bcs6.jsonl\"\n",
    "\n",
    "process_json_file(filepath, max_lines=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "DNEO5I75YRvr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 166,
     "status": "ok",
     "timestamp": 1744568444772,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "DNEO5I75YRvr",
    "outputId": "096e59e6-0498-4c26-aa7a-747e2c0f2910"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '[INST] <<SYS>> You are a helpful assistant. Always answer in the most accurate way. <</SYS>> Write a story about a man on a quest to find the Holy Grail.. [/INST]', 'result': \"  Of course! Here's a story about a man named Sir Timothy on a quest to find the Holy Grail:\\n\\nSir Timothy was a knight in the kingdom of England, known for his bravery and his unwavering devotion to his lord. But despite his many accomplishments, he felt a sense of restlessness within him, a feeling that there was something more he was meant to do.\\n\\nOne day, while on a hunting trip in the forest, Sir Timothy stumbled upon an old man who was tending to a small garden. The old man introduced himself as Sir Jasper, a wise and ancient knight who had been on a quest to find the Holy Grail for many years.\\n\\nSir Jasper told Sir Timothy that the Holy Grail was the cup that Jesus Christ had used during the Last Supper, and that it had the power to heal the sick, bring peace to the troubled, and grant eternal life to those who possessed it. He explained that the Grail was hidden in a far-off land, guarded by fearsome creatures and protected by powerful spells.\\n\\nSir Timothy was immediately captivated by the idea of finding the Holy Grail, and he begged Sir Jasper to take him on as a companion on his quest. Sir Jasper agreed, and the two knights set off on their journey.\\n\\nThey traveled through mountains and valleys, across rivers and deserts, facing many dangers and obstacles along the way. They fought off fierce beasts and overcame treacherous terrain, but Sir Timothy and Sir Jasper remained undeterred.\\n\\nAs they neared the location of the Holy Grail, they encountered a beautiful maiden named Lady Isabelle, who had been sent by fate to guide them on their journey. She revealed that the Grail was hidden in a secret chamber deep within a castle, guarded by a fearsome dragon.\\n\\nSir Timothy and Sir Jasper bravely approached the castle, armed with nothing but their swords and their faith. They fought the dragon in a fierce battle, but as they were about to emerge victorious, the dragon unleashed a powerful spell that sent them both tumbling to the ground.\\n\\nAs Sir Timothy and Sir Jasper lay there, they saw a vision of the Holy Grail hovering above them, shining with a brilliant light. The Grail spoke to them in a voice that echoed in their minds, telling them that they had proven themselves worthy by their bravery and their devotion.\\n\\nWith the Grail in their possession, Sir Timothy and Sir Jasper returned to their kingdom, where they were greeted as heroes. They used the Grail's power to heal the sick and bring peace to the troubled, and they lived out the rest of their days in happiness and fulfillment.\\n\\nAnd so, the legend of Sir Timothy and Sir Jasper's quest for the Holy Grail lived on, inspiring generations to come after them.\", 'ppl': 4.4765625}\n",
      "{'prompt': '[INST] <<SYS>> You are a helpful assistant. Always answer in the most accurate way. <</SYS>> Write a story about two college friends falling in love.. [/INST]', 'result': '  Once upon a time, in the bustling campus of a prestigious university, two students, Rachel and Ryan, met and became inseparable friends. They were both sophomores, majoring in English literature and economics, respectively.\\n\\nRachel was a bright and ambitious student, always eager to learn and explore new ideas. She had long, curly brown hair and big brown eyes that sparkled with curiosity. Ryan, on the other hand, was a bit more laid back, with a mop of messy blond hair and a quick wit that often left his friends in stitches.\\n\\nDespite their different personalities, Rachel and Ryan hit it off immediately. They bonded over their shared love of literature, music, and obscure pop culture references. They spent countless hours discussing everything from Shakespearean sonnets to the latest indie rock bands.\\n\\nAs the semester progressed, their friendship deepened, and they found themselves spending more and more time together. They would grab coffee in the student union, study for exams together in the library, and even go on impromptu adventures around campus, exploring hidden corners and secret spots that only they knew existed.\\n\\nOne day, as they were walking back to their dorms after a long day of classes, Ryan suddenly stopped and turned to Rachel. \"I have to tell you something,\" he said, his eyes locked on hers. \"I\\'ve been feeling this way for a while now, but I couldn\\'t find the right words. The truth is, I think I might be falling in love with you.\"\\n\\nRachel\\'s heart skipped a beat as she met his gaze. She had been feeling the same way for weeks, but had been afraid to admit it. \"I think I might feel the same way,\" she whispered, her voice barely audible.\\n\\nRyan\\'s face lit up with joy, and he pulled Rachel into a tight embrace. They hugged for what felt like an eternity, their hearts pounding with excitement and disbelief.\\n\\nFrom that day on, Rachel and Ryan were more than just friends. They were inseparable, spending every waking moment together. They studied for exams, wrote papers, and even went on road trips, always with a deep sense of love and appreciation for each other.\\n\\nAs graduation approached, Rachel and Ryan knew that their time together would soon come to an end. They had talked about their plans to stay in touch, but they both knew that nothing could replace the magic of their college days.\\n\\nAs they sat on a bench in the campus quad, watching the sun set behind the buildings, Ryan turned to Rachel and said, \"I\\'m so glad we met. You\\'ve made my time here unforgettable.\"\\n\\nTears pricked at the corners of Rachel\\'s eyes as she replied, \"I\\'m glad too. You\\'ve made me feel seen and heard in a way that I never thought possible.\"\\n\\nAs they held hands and watched the sky turn pink and gold, they both knew that their love was something special, something that would last a lifetime. And as they walked away from their college days, hand in hand, they knew that their bond would only grow stronger with time.', 'ppl': 4.6328125}\n",
      "{'prompt': '[INST] <<SYS>> You are a helpful assistant. Always answer in the most accurate way. <</SYS>> Write a story about a policeman saving a building held hostage by group of terrorists.. [/INST]', 'result': \"  It was a tense standoff as the group of terrorists, armed and dangerous, held the building hostage in the heart of the city. The police had been called to the scene and were doing their best to negotiate with the terrorists, but it was clear that they were not going to back down without a fight.\\n\\nOfficer Jameson, a seasoned policeman with years of experience, was tasked with leading the rescue mission. He knew that it would be a dangerous operation, but he was determined to save the innocent people being held hostage.\\n\\nAs he and his team approached the building, they could hear the terrorists shouting demands and threats through the walls. Officer Jameson knew that they had to act quickly, before the situation escalated any further.\\n\\nWith a deep breath, he led his team into the building, moving stealthily through the hallways. They could hear the terrorists moving around, unaware of their presence, and Officer Jameson knew that they had to be ready to move at a moment's notice.\\n\\nSuddenly, one of the terrorists saw them and shouted a warning to the others. Officer Jameson and his team were caught in a hail of gunfire, but they refused to back down. They returned fire, using their training and skills to take down the terrorists one by one.\\n\\nAs the gunfire died down, Officer Jameson and his team cautiously moved through the building, searching for any remaining terrorists. They found two of them hiding in a conference room, and after a tense standoff, they were able to apprehend them without harming them.\\n\\nThe building was safe, and the hostages were finally free. Officer Jameson and his team had done it – they had saved the day, and the city was safe once again.\\n\\nAs the media swarmed around him, Officer Jameson couldn't help but feel a sense of pride and satisfaction. He had done his job, and he had done it well. He knew that he and his team had made a difference, and that the people of the city were grateful for their bravery and sacrifice.\\n\\nFor Officer Jameson, it was just another day on the job, but for the people of the city, it was a reminder of the courage and dedication of the men and women who protected them.\", 'ppl': 4.1796875}\n"
     ]
    }
   ],
   "source": [
    "filepath = \"results/benchmark/Llama-2-7b-chat-hf/results_ppl_invented_stories_926_sentence-wm_2_2_1024_6_1.0_opt-2.7b_bcs6.jsonl\"\n",
    "\n",
    "process_json_file(filepath, max_lines=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4g7bz1Q8g6DF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56219,
     "status": "ok",
     "timestamp": 1744568160150,
     "user": {
      "displayName": "Burhan Maseel",
      "userId": "04282148287258788287"
     },
     "user_tz": 240
    },
    "id": "4g7bz1Q8g6DF",
    "outputId": "c2b9e8d5-ccb0-415e-e6bc-2a619acb77d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-13 18:15:11.984177: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-13 18:15:12.002486: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744568112.024927   20831 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744568112.031636   20831 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-13 18:15:12.054042: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "./results/benchmark/Llama-2-7b-chat-hf ['fake_news', 'story_reports', 'invented_stories']\n",
      "Clean text path:  results_fake_news_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\n",
      "./results/benchmark/Llama-2-7b-chat-hf/results_fake_news_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\n",
      "MAUVE sentence-wm/2: {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
      "Saving in: ./results/benchmark/Llama-2-7b-chat-hf/rouge_fake_news_926_sentence-wm_2_2_1024_6_1.0_bcs6.pkl\n",
      "Clean text path:  results_story_reports_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\n",
      "./results/benchmark/Llama-2-7b-chat-hf/results_story_reports_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\n",
      "MAUVE sentence-wm/2: {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
      "Saving in: ./results/benchmark/Llama-2-7b-chat-hf/rouge_story_reports_926_sentence-wm_2_2_1024_6_1.0_bcs6.pkl\n",
      "Clean text path:  results_invented_stories_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\n",
      "./results/benchmark/Llama-2-7b-chat-hf/results_invented_stories_926_sentence-wm_2_2_1024_6_1.0_bcs6.jsonl\n",
      "MAUVE sentence-wm/2: {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
      "Saving in: ./results/benchmark/Llama-2-7b-chat-hf/rouge_invented_stories_926_sentence-wm_2_2_1024_6_1.0_bcs6.pkl\n"
     ]
    }
   ],
   "source": [
    "!python src/compute_rouge.py --wm sentence-wm --seed 926 --ngram 6 --model_name Llama-2-7b-chat-hf  --param1 2 --param2 2 --benches fake_news story_reports invented_stories --beam_chunk_size 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cb5159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def process_pickle_file(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            formatted_data = {k: float(v) for k, v in data.items()}\n",
    "            print(formatted_data)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "    except pickle.UnpicklingError as e:\n",
    "        print(f\"Error unpickling file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b89b3da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n"
     ]
    }
   ],
   "source": [
    "filepath = \"results/benchmark/Llama-2-7b-chat-hf/rouge_fake_news_926_sentence-wm_2_2_1024_6_1.0_bcs6.pkl\"\n",
    "\n",
    "process_pickle_file(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2eec8ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n"
     ]
    }
   ],
   "source": [
    "filepath = \"results/benchmark/Llama-2-7b-chat-hf/rouge_invented_stories_926_sentence-wm_2_2_1024_6_1.0_bcs6.pkl\"\n",
    "\n",
    "process_pickle_file(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "800395a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n"
     ]
    }
   ],
   "source": [
    "filepath = \"results/benchmark/Llama-2-7b-chat-hf/rouge_story_reports_926_sentence-wm_2_2_1024_6_1.0_bcs6.pkl\"\n",
    "\n",
    "process_pickle_file(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bee91a",
   "metadata": {},
   "source": [
    "# 5. Experimental Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d4e8b",
   "metadata": {},
   "source": [
    "# 6. Conclusion and Future Direction\n",
    "\n",
    "\n",
    "## 6.1. Conclusion\n",
    "WaterMax stands out as a new approach to watermarking text generated by large language models (LLMs). Unlike earlier methods, it starts by designing a strong detection system first, and then builds a generation process that makes the watermark easy to spot — without changing anything inside the LLM.\n",
    "This smart design brings several key benefits:\n",
    "- High Quality: The generated text still reads naturally and fluently.\n",
    "- Strong Detection: The watermark is easy to detect, even in short texts.\n",
    "- Robustness: It holds up well against paraphrasing and small edits.\n",
    "- No LLM Changes: It works with any LLM (even closed ones like ChatGPT) without needing internal access.\n",
    "\n",
    "\n",
    "## 6.2. Future Direction\n",
    "A key direction for future work is the distillation of WaterMax — training or fine-tuning a language model to natively generate watermarked text. This would eliminate the current computational overhead of generating multiple drafts and make the watermarking process faster and more efficient.\n",
    "If successful, this approach could allow real-time, low-cost watermarking while keeping all the benefits of WaterMax — high fluency, strong detection, and compatibility with any LLM.\n",
    "\n",
    "\n",
    "## 6.3. Lessons Learned\n",
    "- Learned how to clone and set up a GitHub project locally.\n",
    "- Understood the overall structure and logic of the WaterMax codebase.\n",
    "- Gained hands-on experience in running the project and interpreting the output.\n",
    "- Developed a deeper understanding of how watermarking works in LLMs through practical exploration.\n",
    "- Realized the importance of detector-first design and chunk-level sampling in watermark robustness.\n",
    "\n",
    "\n",
    "# 7. References:\n",
    "[1]: Giboulot, E., & Furon, T. (2024, December 11). WaterMax: Breaking the LLM watermark detectability-robustness-quality trade-off. Advances in Neural Information Processing Systems, 37 (NeurIPS 2024) Main Conference Track, https://openreview.net/forum?id=HjeKHxK2VH.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "watermax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
